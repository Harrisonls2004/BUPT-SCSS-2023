#coding=utf-8
"""
=================================================================
ä»Šæ—¥å¤´æ¡è´Ÿé¢èˆ†æƒ…æ™ºèƒ½ç›‘æµ‹ç³»ç»Ÿ
=================================================================

ç³»ç»ŸåŠŸèƒ½ï¼š
1. æ•°æ®é‡‡é›†ï¼šæ”¯æŒæŒ‰ä¸»é¢˜å…³é”®è¯ã€ç”¨æˆ·IDã€æ¨èå†…å®¹ç­‰å¤šç§æ–¹å¼çˆ¬å–ä»Šæ—¥å¤´æ¡æ•°æ®
2. æ•°æ®å­˜å‚¨ï¼šå°†çˆ¬å–çš„å¸–å­ã€è¯„è®ºã€å›å¤ç­‰æ•°æ®ç»“æ„åŒ–å­˜å‚¨åˆ°SQL Serveræ•°æ®åº“
3. è´Ÿé¢èˆ†æƒ…æ£€æµ‹ï¼šåŸºäº6å¤§é¢†åŸŸï¼ˆæ¶‰æ”¿æœ‰å®³ã€ä¾®è¾±è°©éª‚ã€è‰²æƒ…æš´åŠ›ã€äº‹æ•…ç¾éš¾ã€èšé›†ç»´æƒã€å¨±ä¹å…«å¦ï¼‰
   æ¯ä¸ªé¢†åŸŸ50+å…³é”®è¯çš„æ™ºèƒ½æ£€æµ‹ç®—æ³•
4. é£é™©è¯„ä¼°ï¼šå¤šç»´åº¦è¯„åˆ†æœºåˆ¶ï¼ŒåŒ…æ‹¬å…³é”®è¯æƒé‡ã€ç»„åˆé€»è¾‘ã€æƒ…æ„Ÿå¼ºåº¦ç­‰
5. ç»Ÿè®¡åˆ†æï¼šè¯¦ç»†çš„ç»Ÿè®¡æŠ¥å‘Šï¼ŒåŒ…æ‹¬åˆ†å¸ƒåˆ†æã€é£é™©ç­‰çº§ã€è¶‹åŠ¿é¢„æµ‹ç­‰
6. å¯è§†åŒ–å±•ç¤ºï¼š6ç§å›¾è¡¨å…¨æ–¹ä½å±•ç¤ºåˆ†æç»“æœ
7. ç»“æœå¯¼å‡ºï¼šç”ŸæˆCSVæ ¼å¼çš„è¯¦ç»†åˆ†ææŠ¥å‘Š

è®¾è®¡ç†å¿µï¼š
- è´Ÿé¢èˆ†æƒ…å®šä¹‰ï¼šèƒ½å¤Ÿå¼•èµ·å…¬ä¼—è´Ÿé¢æƒ…ç»ªã€ç ´åç¤¾ä¼šå’Œè°ã€å½±å“æ”¿åºœå½¢è±¡çš„ç½‘ç»œè¨€è®º
- æ£€æµ‹ç­–ç•¥ï¼šå…³é”®è¯åŒ¹é… + è¯­ä¹‰åˆ†æ + é£é™©è¯„ä¼° + äººå·¥æ™ºèƒ½ç®—æ³•
- åˆ†çº§ç®¡ç†ï¼šé«˜ã€ä¸­ã€ä½ä¸‰çº§é£é™©åˆ†ç±»ï¼Œä¾¿äºä¼˜å…ˆçº§å¤„ç†
- å®æ—¶ç›‘æ§ï¼šæ”¯æŒå¤§è§„æ¨¡æ•°æ®å¤„ç†å’Œå®æ—¶åˆ†æ

æŠ€æœ¯æ ˆï¼š
- æ•°æ®é‡‡é›†ï¼šSelenium + Requests
- æ•°æ®åº“ï¼šSQL Server + pyodbc
- åˆ†æç®—æ³•ï¼šPython + è‡ªç„¶è¯­è¨€å¤„ç†
- å¯è§†åŒ–ï¼šMatplotlib + å¤šå›¾è¡¨å±•ç¤º
- å¯¼å‡ºï¼šCSVæ ¼å¼æ ‡å‡†åŒ–æŠ¥å‘Š

ä½œè€…ï¼šææ˜Šä¼¦
ç‰ˆæœ¬ï¼š2.0 (æ™ºèƒ½å¢å¼ºç‰ˆ)
æ—¥æœŸï¼š2025å¹´6æœˆ26æ—¥
=================================================================
"""

import requests
from urllib.parse import urlencode
import os
import pyodbc  # ç”¨äºè¿æ¥SQL Server
import time
import matplotlib.pyplot as plt
import numpy as np
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.keys import Keys
import re
import urllib.parse
import random
from wordcloud import WordCloud
import jieba
from collections import Counter

# SQL Serverè¿æ¥ä¿¡æ¯ï¼ˆè¯·æ ¹æ®å®é™…æƒ…å†µå¡«å†™ï¼‰
# ç¤ºä¾‹ï¼šserver = '127.0.0.1', database = 'toutiao', username = 'sa', password = 'yourpassword'
SQL_SERVER = '127.0.0.1'
SQL_DATABASE = 'toutiao'
SQL_USERNAME = 'sa'
SQL_PASSWORD = '12lhl0408'
CONN_STR = f'DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={SQL_SERVER};DATABASE={SQL_DATABASE};UID={SQL_USERNAME};PWD={SQL_PASSWORD}'

# é€šç”¨è¯·æ±‚å¤´ï¼ˆè¯·å°†Cookieæ›¿æ¢ä¸ºä½ è‡ªå·±çš„æµè§ˆå™¨Cookieï¼‰
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
    'Accept-Encoding': 'gzip, deflate, br',
    'Cache-Control': 'no-cache',
    'Pragma': 'no-cache',
    'Sec-Ch-Ua': '"Google Chrome";v="123", "Not:A-Brand";v="8", "Chromium";v="123"',
    'Sec-Ch-Ua-Mobile': '?0',
    'Sec-Ch-Ua-Platform': '"Windows"',
    'Sec-Fetch-Dest': 'document',
    'Sec-Fetch-Mode': 'navigate',
    'Sec-Fetch-Site': 'none',
    'Sec-Fetch-User': '?1',
    'Upgrade-Insecure-Requests': '1',
    'Cookie': '__ac_signature=_02B4Z6wo00f010nHwjQAAIDBqggyoWEwlItJ58aAALo178; tt_webid=7519857288368293386; gfkadpd=24,6457; ttcid=e2e7b32c63ca4802a4a49330ca717f4836; local_city_cache=%E5%8C%97%E4%BA%AC; s_v_web_id=verify_mcbwt1iz_oBDKiQ4w_3TTo_4AZ1_8pow_r9zubGMABFAy; csrftoken=93f6e1f548351580e6a5429fca17dd56; _ga=GA1.1.1790194237.1750853229; passport_csrf_token=a5de579da71241a6a0cf84c0e313d6cc; passport_csrf_token_default=a5de579da71241a6a0cf84c0e313d6cc; passport_mfa_token=Cjc7ku%2FJQw7jdzq7Ucla52gz%2FYKoWc%2BgD7W6gq1r2ATh8nyJ1kxllGldP4fFATdPU1%2FLtgQvq11EGkoKPAAAAAAAAAAAAABPKKvLaxJrHK%2FSus65uJ14ixgZG4et%2F3IJ%2BnxT4me7J97JhGQfLBVfnxlZBg5tzlNNexDAh%2FUNGPax0WwgAiIBA64dO78%3D; d_ticket=34050ff1b9a466e07040a648be3e62583065c; n_mh=3bj4nRzRoXjC5RCx-DcOggwnWm7DfJmjOj8GghVtbiI; sso_auth_status=f1dea548ac625692288bc26e757a9e1f; sso_auth_status_ss=f1dea548ac625692288bc26e757a9e1f; sso_uid_tt=2e2b60717454959027f0f8b97fe5402e; sso_uid_tt_ss=2e2b60717454959027f0f8b97fe5402e; toutiao_sso_user=3d0d3d051540a33753036a64c85f529c; toutiao_sso_user_ss=3d0d3d051540a33753036a64c85f529c; sid_ucp_sso_v1=1.0.0-KDkzYjRjMDI0MTMwN2JmNTE3NjAyZTE4ZWEzMDhlYTNjZWZkYjlhNmYKHgjY5ZDmmfX1BRChzu_CBhgYIAww9pvn9AU4AkDxBxoCaGwiIDNkMGQzZDA1MTU0MGEzMzc1MzAzNmE2NGM4NWY1Mjlj; ssid_ucp_sso_v1=1.0.0-KDkzYjRjMDI0MTMwN2JmNTE3NjAyZTE4ZWEzMDhlYTNjZWZkYjlhNmYKHgjY5ZDmmfX1BRChzu_CBhgYIAww9pvn9AU4AkDxBxoCaGwiIDNkMGQzZDA1MTU0MGEzMzc1MzAzNmE2NGM4NWY1Mjlj; passport_auth_status=d1808c75323c2f68ddaa1ab41805b317%2C6b1992d6ac3c6791fe870548c26832b4; passport_auth_status_ss=d1808c75323c2f68ddaa1ab41805b317%2C6b1992d6ac3c6791fe870548c26832b4; sid_guard=39dbca9daa6870c51a33f9cfaff2b987%7C1750853409%7C5184002%7CSun%2C+24-Aug-2025+12%3A10%3A11+GMT; uid_tt=81576c0f45914e86a124aec5ccba6901; uid_tt_ss=81576c0f45914e86a124aec5ccba6901; sid_tt=39dbca9daa6870c51a33f9cfaff2b987; sessionid=39dbca9daa6870c51a33f9cfaff2b987; sessionid_ss=39dbca9daa6870c51a33f9cfaff2b987; is_staff_user=false; sid_ucp_v1=1.0.0-KGU3YWFkMTM5YzcyMzJlNzgwNzViMzU5NzM3NzIxZGI3NGY1OGZkNWQKGAjY5ZDmmfX1BRChzu_CBhgYIAw4AkDxBxoCbGYiIDM5ZGJjYTlkYWE2ODcwYzUxYTMzZjljZmFmZjJiOTg3; ssid_ucp_v1=1.0.0-KGU3YWFkMTM5YzcyMzJlNzgwNzViMzU5NzM3NzIxZGI3NGY1OGZkNWQKGAjY5ZDmmfX1BRChzu_CBhgYIAw4AkDxBxoCbGYiIDM5ZGJjYTlkYWE2ODcwYzUxYTMzZjljZmFmZjJiOTg3; odin_tt=528c4b798b9a4cdc0b5caeed44bd4aa5b6cacdd65f03fc3b0c25cf0c5ba6f7ccabd622f3cdc6aa2f79c5c0c091079ba5; ttwid=1%7CB7nV5kvCOgIGd5L6INIiX5vVnL7m_Xq3Pi9px9ovOJ4%7C1750930094%7C3ea35f13bda142b1c8361eaa3d606da5c33deb93c9f2ab8d1f5cf2fc5298b7e5; _ga_QEHZPBE5HH=GS2.1.s1750930093$o8$g0$t1750930093$j60$l0$h0; tt_anti_token=5J0x44Ql-896df79be955ea9a31d87e73cfc3246be2fcdc45d5e66bd31395644c76f52de5; tt_scid=ZifjN9cmNKQXmAwIA-LwA5rFc5EYuC7X8O90RBEnxdpIx2KjAlsiPhfE48OMYmW1730c',  # ç”¨æµè§ˆå™¨F12æŠ“åŒ…è·å–
    'Referer': 'https://www.toutiao.com/'
}

# æ•°æ®åº“åˆå§‹åŒ–
# SQL Serverè¡¨ç»“æ„ç•¥æœ‰ä¸åŒï¼Œä¸»é”®ç±»å‹ä¸ºNVARCHARï¼Œparent_idå…è®¸NULL
# è‹¥è¡¨å·²å­˜åœ¨ä¸ä¼šæŠ¥é”™

def init_db():
    conn = pyodbc.connect(CONN_STR)
    c = conn.cursor()
    c.execute('''
        IF NOT EXISTS (SELECT * FROM sysobjects WHERE name='posts' AND xtype='U')
        CREATE TABLE posts (
            id NVARCHAR(64) PRIMARY KEY,
            title NVARCHAR(512),
            content NVARCHAR(MAX),
            user_id NVARCHAR(64)
        )''')
    c.execute('''
        IF NOT EXISTS (SELECT * FROM sysobjects WHERE name='comments' AND xtype='U')
        CREATE TABLE comments (
            id NVARCHAR(64) PRIMARY KEY,
            post_id NVARCHAR(64),
            user_id NVARCHAR(64),
            content NVARCHAR(MAX),
            parent_id NVARCHAR(64) NULL
        )''')
    conn.commit()
    conn.close()

# ä¿å­˜å¸–å­ï¼Œä¼ é€’æ•°æ®åº“è¿æ¥å¯¹è±¡

def save_post(conn, post):
    c = conn.cursor()
    # ä½¿ç”¨MERGEè¯­å¥é¿å…é‡å¤æ’å…¥ï¼ŒåŒæ—¶æ›´æ–°å†…å®¹
    c.execute('''
        MERGE posts AS target
        USING (SELECT ? AS id, ? AS title, ? AS content, ? AS user_id) AS source
        ON target.id = source.id
        WHEN NOT MATCHED THEN
            INSERT (id, title, content, user_id) 
            VALUES (source.id, source.title, source.content, source.user_id)
        WHEN MATCHED THEN
            UPDATE SET 
                title = CASE WHEN LEN(source.title) > LEN(target.title) THEN source.title ELSE target.title END,
                content = CASE WHEN LEN(source.content) > LEN(target.content) THEN source.content ELSE target.content END;
    ''', post['id'], post['title'], post.get('content', ''), post['user_id'])

# ä¿å­˜è¯„è®ºï¼Œä¼ é€’æ•°æ®åº“è¿æ¥å¯¹è±¡

def save_comment(conn, comment):
    c = conn.cursor()
    # ä½¿ç”¨MERGEè¯­å¥é¿å…é‡å¤æ’å…¥è¯„è®º
    c.execute('''
        MERGE comments AS target
        USING (SELECT ? AS id, ? AS post_id, ? AS user_id, ? AS content, ? AS parent_id) AS source
        ON target.id = source.id
        WHEN NOT MATCHED THEN
            INSERT (id, post_id, user_id, content, parent_id) 
            VALUES (source.id, source.post_id, source.user_id, source.content, source.parent_id);
    ''', comment['id'], comment['post_id'], comment['user_id'], comment['content'], comment['parent_id'])

# æŒ‰ä¸»é¢˜çˆ¬å–å‰100ä¸ªå¸–å­
def get_posts_by_keyword(keyword, total=100):
    """
    ä½¿ç”¨requestsæ–¹å¼æœç´¢ä»Šæ—¥å¤´æ¡å†…å®¹ï¼ˆå¤‡é€‰æ–¹æ¡ˆï¼‰
    """
    posts = []
    seen_ids = set()
    
    # å°è¯•å¤šç§APIæ¥å£
    api_urls = [
        'https://www.toutiao.com/api/search/content/',
        'https://www.toutiao.com/search_content/',
        'https://m.toutiao.com/search_content/'
    ]
    
    for api_url in api_urls:
        print(f"å°è¯•API: {api_url}")
        for offset in range(0, total, 20):
            if len(posts) >= total:
                break
                
            # æ„å»ºå‚æ•°
            params = {
                'offset': offset,
                'format': 'json',
                'keyword': keyword,
                'autoload': 'true',
                'count': '20',
                'cur_tab': '1',
                'from': 'search_tab',
                'pd': 'synthesis',
                'source': 'input'
            }
            
            # å¤‡é€‰å‚æ•°ç»„åˆ
            if 'api/search' in api_url:
                params = {
                    'keyword': keyword,
                    'offset': offset,
                    'count': 20,
                    'format': 'json',
                    'autoload': 'true'
                }
            
            url = api_url + '?' + urlencode(params)
            
            try:
                response = requests.get(url, headers=HEADERS, timeout=10)
                print(f"è¯·æ±‚URL: {url[:100]}...")
                print(f"çŠ¶æ€ç : {response.status_code}")
                
                if response.status_code == 200:
                    try:
                        json_data = response.json()
                        data = json_data.get('data')
                        
                        if not data and 'result' in json_data:
                            data = json_data['result']
                        if not data and 'results' in json_data:
                            data = json_data['results']
                            
                        if data:
                            print(f"è·å–åˆ° {len(data)} æ¡åŸå§‹æ•°æ®")
                            
                            for item in data:
                                if not isinstance(item, dict):
                                    continue
                                    
                                title = item.get('title', '').strip()
                                group_id = item.get('group_id') or item.get('id') or item.get('article_id')
                                
                                if title and group_id and str(group_id) not in seen_ids:
                                    post = {
                                        'id': str(group_id),
                                        'title': title,
                                        'content': item.get('abstract', '') or item.get('summary', '') or item.get('content', ''),
                                        'user_id': str(item.get('user_id', '') or item.get('author_id', ''))
                                    }
                                    posts.append(post)
                                    seen_ids.add(str(group_id))
                                    
                                    if len(posts) >= total:
                                        break
                        else:
                            print("æœªæ‰¾åˆ°æ•°æ®å­—æ®µ")
                            print(f"å“åº”ç»“æ„: {list(json_data.keys()) if isinstance(json_data, dict) else 'not dict'}")
                            
                    except Exception as e:
                        print(f"è§£æJSONå¤±è´¥: {e}")
                        print(f"å“åº”å†…å®¹å‰500å­—ç¬¦: {response.text[:500]}")
                else:
                    print(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                    
            except Exception as e:
                print(f"è¯·æ±‚å¼‚å¸¸: {e}")
                
            time.sleep(2)  # å¢åŠ å»¶è¿Ÿé¿å…è¢«é™åˆ¶
            
        if posts:  # å¦‚æœå·²ç»è·å–åˆ°æ•°æ®ï¼Œå°±ä¸å°è¯•å…¶ä»–APIäº†
            break
    
    print(f"requestsæ–¹å¼æœ€ç»ˆè·å–åˆ° {len(posts)} æ¡æ•°æ®")
    return posts

# æŒ‰ç”¨æˆ·IDçˆ¬å–æ‰€æœ‰å¸–å­
def get_posts_by_user(user_id, max_pages=10):
    posts = []
    for page in range(max_pages):
        url = f'https://www.toutiao.com/c/user/article/?user_id={user_id}&max_behot_time={(int(time.time())-page*1000)}'
        response = requests.get(url, headers=HEADERS)
        print("è¯·æ±‚URL:", url)
        print("çŠ¶æ€ç :", response.status_code)
        print("è¿”å›å†…å®¹:", response.text[:500])
        if response.status_code == 200:
            try:
                data = response.json().get('data')
            except Exception as e:
                print("è§£æJSONå¤±è´¥:", e)
                data = None
            if data:
                for item in data:
                    if item.get('title') and item.get('group_id'):
                        post = {
                            'id': str(item['group_id']),
                            'title': item['title'],
                            'content': item.get('abstract', ''),
                            'user_id': user_id
                        }
                        posts.append(post)
        time.sleep(1)
    return posts

# çˆ¬å–è¯„è®ºå’Œå›å¤ï¼Œä¿ç•™å›å¤å…³ç³»
def get_comments(post_id):
    """
    è·å–å¸–å­è¯„è®ºï¼Œå°è¯•å¤šç§APIæ¥å£
    """
    comments = []
    
    # å°è¯•å¤šç§è¯„è®ºAPI
    api_urls = [
        f'https://www.toutiao.com/api/comment/list/?group_id={post_id}&item_id={post_id}&count=50',
        f'https://m.toutiao.com/api/comment/list/?group_id={post_id}&item_id={post_id}&count=50',
        f'https://www.toutiao.com/article/v2/tab_comments/?group_id={post_id}&item_id={post_id}&count=50',
        f'https://www.toutiao.com/api/pc/list_comment/?group_id={post_id}&item_id={post_id}&count=50'
    ]
    
    for api_url in api_urls:
        try:
            print(f"ğŸ” å°è¯•è·å–è¯„è®º: {post_id}")
            response = requests.get(api_url, headers=HEADERS, timeout=10)
            
            if response.status_code == 200:
                try:
                    json_data = response.json()
                    
                    # å°è¯•ä¸åŒçš„æ•°æ®ç»“æ„
                    data = None
                    if 'data' in json_data:
                        if 'comments' in json_data['data']:
                            data = json_data['data']['comments']
                        elif isinstance(json_data['data'], list):
                            data = json_data['data']
                    elif 'comments' in json_data:
                        data = json_data['comments']
                    elif isinstance(json_data, list):
                        data = json_data
                    
                    if data and isinstance(data, list):
                        print(f"âœ… è·å–åˆ°{len(data)}æ¡è¯„è®º")
                        
                        for item in data:
                            if not isinstance(item, dict):
                                continue
                                
                            comment_id = str(item.get('id', '') or item.get('comment_id', ''))
                            content = item.get('text', '') or item.get('content', '') or item.get('comment_text', '')
                            user_id = str(item.get('user_id', '') or item.get('author_id', ''))
                            
                            if comment_id and content:
                                comment = {
                                    'id': comment_id,
                                    'post_id': post_id,
                                    'user_id': user_id,
                                    'content': content[:500],  # é™åˆ¶é•¿åº¦
                                    'parent_id': None
                                }
                                comments.append(comment)
                                
                                # è·å–å›å¤
                                replies = item.get('reply_list', []) or item.get('replies', [])
                                if replies:
                                    for reply in replies:
                                        if not isinstance(reply, dict):
                                            continue
                                            
                                        reply_id = str(reply.get('id', '') or reply.get('reply_id', ''))
                                        reply_content = reply.get('text', '') or reply.get('content', '') or reply.get('reply_text', '')
                                        reply_user = str(reply.get('user_id', '') or reply.get('author_id', ''))
                                        
                                        if reply_id and reply_content:
                                            reply_comment = {
                                                'id': reply_id,
                                                'post_id': post_id,
                                                'user_id': reply_user,
                                                'content': reply_content[:500],
                                                'parent_id': comment_id
                                            }
                                            comments.append(reply_comment)
                        
                        if comments:  # å¦‚æœè·å–åˆ°è¯„è®ºå°±ä¸å°è¯•å…¶ä»–APIäº†
                            break
                    else:
                        print(f"âŒ APIå“åº”æ ¼å¼ä¸æ­£ç¡®: {api_url}")
                        
                except Exception as e:
                    print(f"âŒ è§£æè¯„è®ºJSONå¤±è´¥: {e}")
                    continue
            else:
                print(f"âŒ è¯„è®ºè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                
        except Exception as e:
            print(f"âŒ è¯„è®ºè¯·æ±‚å¼‚å¸¸: {e}")
            continue
    
    if not comments:
        print(f"âš ï¸  æœªè·å–åˆ°å¸–å­{post_id}çš„è¯„è®º")
    else:
        print(f"ğŸ“ æˆåŠŸè·å–{len(comments)}æ¡è¯„è®ºå’Œå›å¤")
        
    return comments

# è´Ÿé¢èˆ†æƒ…å…³é”®è¯è¡¨ - æ¯ä¸ªé¢†åŸŸä¸å°‘äº50ä¸ªå…³é”®è¯
NEGATIVE_KEYWORDS = {
    'æ¶‰æ”¿æœ‰å®³': [
        # åæ”¿åºœç›¸å…³
        'åæ”¿åºœ', 'é¢ è¦†æ”¿æƒ', 'æ¨ç¿»æ”¿åºœ', 'æ”¿å˜', 'å›å›½', 'å›ä¹±', 'æš´åŠ¨', 'èµ·ä¹‰', 'é€ å', 'é©å‘½',
        'åä½“åˆ¶', 'åå»ºåˆ¶', 'åå½“å±€', 'åå®˜æ–¹', 'åå›½å®¶', 'åç¤¾ä¼š', 'ååŠ¨', 'é¢ è¦†', 'åˆ†è£‚', 'åˆ†ç¦»',
        # ææ€–ä¸»ä¹‰ç›¸å…³
        'ææ€–ä¸»ä¹‰', 'ææ€–åˆ†å­', 'ææ€–è¢­å‡»', 'æš´æ', 'æš´æåˆ†å­', 'æç«¯ä¸»ä¹‰', 'åœ£æˆ˜', 'è‡ªæ€å¼è¢­å‡»', 'äººè‚‰ç‚¸å¼¹', 'ç”ŸåŒ–æ­¦å™¨',
        # æ°‘æ—åˆ†è£‚ç›¸å…³
        'å°ç‹¬', 'æ¸¯ç‹¬', 'ç–†ç‹¬', 'è—ç‹¬', 'åˆ†è£‚å›½å®¶', 'æ°‘æ—åˆ†è£‚', 'åˆ†è£‚ä¸»ä¹‰', 'åˆ†è£‚åˆ†å­', 'åˆ†è£‚åŠ¿åŠ›', 'åˆ†è£‚æ´»åŠ¨',
        # ç…½åŠ¨ç›¸å…³
        'ç…½åŠ¨', 'ç…½åŠ¨åˆ†è£‚', 'ç…½åŠ¨æš´ä¹±', 'ç…½åŠ¨ä»‡æ¨', 'ç…½åŠ¨å¯¹ç«‹', 'ç…½åŠ¨æƒ…ç»ª', 'é€ è°£', 'ä¼ è°£', 'è°£è¨€', 'å‡æ¶ˆæ¯',
        # è¿æ³•é›†ä¼šç›¸å…³
        'éæ³•é›†ä¼š', 'éæ³•æ¸¸è¡Œ', 'éæ³•ç¤ºå¨', 'éæ³•ç»„ç»‡', 'åœ°ä¸‹ç»„ç»‡', 'é‚ªæ•™', 'æ³•è½®åŠŸ', 'å…¨èƒ½ç¥', 'æš´åŠ›æŠ—æ³•'
    ],
    'ä¾®è¾±è°©éª‚': [
        # è„è¯ç²—å£
        'å‚»é€¼', 'å‚»B', 'SB', 'è‰æ³¥é©¬', 'æ“ä½ å¦ˆ', 'å»æ­»', 'æ­»å…¨å®¶', 'æ»šè›‹', 'æ»šå¼€', 'æ»šè¿œç‚¹',
        'ç‹—å±', 'ç‹—æ‚ç§', 'ç‹—ä¸œè¥¿', 'ç•œç”Ÿ', 'çŒªç‹—', 'çŒªå¤´', 'è ¢çŒª', 'æ­»çŒª', 'è‚¥çŒª', 'åºŸç‰©',
        'åƒåœ¾', 'äººæ¸£', 'è´¥ç±»', 'æ··è›‹', 'ç‹å…«è›‹', 'é¾Ÿå„¿å­', 'æ‚ç§', 'é‡ç§', 'å­½ç§', 'è´±ç§',
        # äººèº«æ”»å‡»
        'ç™½ç—´', 'æ™ºéšœ', 'å¼±æ™º', 'è„‘æ®‹', 'ç¥ç»ç—…', 'ç²¾ç¥ç—…', 'ç–¯å­', 'ç–¯ç‹—', 'ç™è›¤èŸ†', 'ä¸‘å…«æ€ª',
        'è´±äºº', 'å©Šå­', 'å¦“å¥³', 'å°å§', 'é¸¡', 'éªšè´§', 'è¡å¦‡', 'æ·«å¦‡', 'ç ´é‹', 'è‡­å©Šå­',
        # æ¶æ¯’å’’éª‚
        'æ­»å…‰', 'æ–­å­ç»å­™', 'ä¸å¾—å¥½æ­»', 'ä¸‹åœ°ç‹±', 'å¤©æ‰“é›·åŠˆ', 'åƒåˆ€ä¸‡å‰', 'äº”é©¬åˆ†å°¸', 'ç¢å°¸ä¸‡æ®µ', 'æ­»æ— å…¨å°¸', 'é—è‡­ä¸‡å¹´'
    ],
    'è‰²æƒ…æš´åŠ›': [
        # è‰²æƒ…å†…å®¹
        'é»„è‰²', 'é»„ç‰‡', 'è‰²æƒ…', 'æ·«ç§½', 'Aç‰‡', 'AV', 'æˆäººç‰‡', 'æ¯›ç‰‡', 'ä¸‰çº§ç‰‡', 'æƒ…è‰²',
        'è£¸ä½“', 'è£¸ç…§', 'è‰³ç…§', 'æ˜¥å®«', 'åšçˆ±', 'æ€§äº¤', 'æ€§çˆ±', 'å£äº¤', 'è‚›äº¤', 'ç¾¤äº¤',
        'å«–å¨¼', 'å–æ·«', 'æ€§äº¤æ˜“', 'æ€§æœåŠ¡', 'ä¸€å¤œæƒ…', 'çº¦ç‚®', 'å¼€æˆ¿', 'åŒ…å…»', 'æ´äº¤', 'é™ªç¡',
        # æš´åŠ›å†…å®¹
        'æš´åŠ›', 'è¡€è…¥', 'æ®‹å¿', 'è™å¾…', 'é…·åˆ‘', 'æŠ˜ç£¨', 'æ€äºº', 'æ€æˆ®', 'å± æ€', 'ç­é—¨',
        'å¼ºå¥¸', 'è½®å¥¸', 'æ€§ä¾µ', 'çŒ¥äºµ', 'æ€§éªšæ‰°', 'æ€§è™', 'è™ç«¥', 'å®¶æš´', 'æ ¡å›­æš´åŠ›', 'ç½‘ç»œæš´åŠ›',
        'æ‰“æ¶', 'æ–—æ®´', 'æ¢°æ–—', 'ç¾¤æ®´', 'äº’æ®´', 'æ¶æ–—', 'è¡€æ‹¼', 'ç«æ‹¼', 'ä»‡æ€', 'æŠ¥å¤',
        # è‡ªæ®‹è‡ªæ€
        'è‡ªæ€', 'è‡ªæ®‹', 'è‡ªè™', 'è‡ªå®³', 'å‰²è…•', 'è·³æ¥¼', 'ä¸ŠåŠ', 'æœæ¯’', 'çƒ§ç‚­', 'è½»ç”Ÿ'
    ],
    'äº‹æ•…ç¾éš¾': [
        # è‡ªç„¶ç¾å®³
        'åœ°éœ‡', 'æµ·å•¸', 'æ´ªæ°´', 'æ´ªç¾', 'æ°´ç¾', 'æ¶ç¾', 'æ—±ç¾', 'å¹²æ—±', 'å°é£', 'é£“é£',
        'é¾™å·é£', 'æš´é›¨', 'æš´é›ª', 'å†°é›¹', 'é›·å‡»', 'æ³¥çŸ³æµ', 'å±±æ´ª', 'æ»‘å¡', 'åå¡Œ', 'å¡Œæ–¹',
        'ç«å±±', 'ç«å±±çˆ†å‘', 'å²©æµ†', 'åœ°è£‚', 'åœ°é™·', 'æ²™å°˜æš´', 'é›ªå´©', 'å†°ç¾', 'å†»ç¾', 'é«˜æ¸©',
        # äº‹æ•…ç¾éš¾
        'äº‹æ•…', 'ç¾éš¾', 'ç¾å®³', 'å¤§ç«', 'ç«ç¾', 'çˆ†ç‚¸', 'ç‡ƒçˆ†', 'ç“¦æ–¯çˆ†ç‚¸', 'çŸ¿éš¾', 'çŸ¿äº•äº‹æ•…',
        'è½¦ç¥¸', 'äº¤é€šäº‹æ•…', 'æ’è½¦', 'è¿½å°¾', 'ç¿»è½¦', 'ç©ºéš¾', 'å æœº', 'èˆ¹éš¾', 'æ²‰èˆ¹', 'æµ·éš¾',
        'ä¸­æ¯’', 'é£Ÿç‰©ä¸­æ¯’', 'ç…¤æ°”ä¸­æ¯’', 'åŒ–å­¦ä¸­æ¯’', 'çˆ†ç‚¸', 'è§¦ç”µ', 'æººæ°´', 'å è½', 'è¸©è¸', 'æŒ¤å‹',
        # ç–«æƒ…ç›¸å…³
        'ç–«æƒ…', 'ä¼ æŸ“ç—…', 'ç˜Ÿç–«', 'ç—…æ¯’', 'ç»†èŒ', 'æ„ŸæŸ“', 'ä¼ æŸ“', 'æ‰©æ•£', 'çˆ†å‘', 'æµè¡Œç—…'
    ],
    'èšé›†ç»´æƒ': [
        # ç»´æƒæ´»åŠ¨
        'ç»´æƒ', 'ä¸Šè®¿', 'è¯·æ„¿', 'ç”³è¯‰', 'æŠ•è¯‰', 'ä¸¾æŠ¥', 'æ§å‘Š', 'èµ·è¯‰', 'è¯‰è®¼', 'æ‰“å®˜å¸',
        'è®¨è–ª', 'è®¨å€º', 'è¿½å€º', 'è¦è´¦', 'å‚¬æ¬¾', 'æ‹–æ¬ ', 'æ¬ è–ª', 'è¡€æ±—é’±', 'å·¥èµ„', 'èµ”å¿',
        'æ‹†è¿', 'å¾åœ°', 'å¼ºæ‹†', 'æš´åŠ›æ‹†è¿', 'è¿æ³•æ‹†è¿', 'é‡è›®æ‹†è¿', 'é’‰å­æˆ·', 'æ‹†è¿æˆ·', 'å¤±åœ°å†œæ°‘', 'æ‹†äºŒä»£',
        # é›†ä½“è¡ŒåŠ¨
        'æ¸¸è¡Œ', 'ç¤ºå¨', 'é›†ä¼š', 'æŠ—è®®', 'é™å', 'ç½¢å·¥', 'ç½¢è¯¾', 'ç½¢å¸‚', 'ç½¢è¿', 'strikes',
        'å µè·¯', 'å µé—¨', 'å µè½¦', 'å›´å µ', 'å›´æ”»', 'å†²å‡»', 'å é¢†', 'èšä¼—', 'ç¾¤è®¿', 'é›†è®¿',
        'ç¾¤ä½“äº‹ä»¶', 'çªå‘äº‹ä»¶', 'ä¸ç¨³å®šå› ç´ ', 'ç¤¾ä¼šçŸ›ç›¾', 'å®˜æ°‘å†²çª', 'è­¦æ°‘å†²çª', 'å¹²ç¾¤çŸ›ç›¾', 'åŠ³èµ„çº çº·', 'åŒ»æ‚£å†²çª', 'åŸç®¡æ‰§æ³•',
        # æƒåˆ©è¯‰æ±‚
        'äººæƒ', 'æ°‘æƒ', 'å…¬æ°‘æƒ', 'é€‰ä¸¾æƒ', 'çŸ¥æƒ…æƒ', 'ç›‘ç£æƒ', 'è¨€è®ºè‡ªç”±', 'æ–°é—»è‡ªç”±', 'ç»“ç¤¾è‡ªç”±', 'æ°‘ä¸»'
    ],
    'å¨±ä¹å…«å¦': [
        # æ„Ÿæƒ…çº è‘›
        'å‡ºè½¨', 'å©šå¤–æƒ…', 'å·æƒ…', 'åŠˆè…¿', 'è„šè¸ä¸¤èˆ¹', 'ä¸‰è§’æ‹', 'å¤šè§’æ‹', 'å°ä¸‰', 'å°å››', 'æƒ…äºº',
        'åˆ†æ‰‹', 'ç¦»å©š', 'å¤åˆ', 'å’Œå¥½', 'åˆ†å±…', 'å†·æˆ˜', 'åµæ¶', 'äº‰åµ', 'çŸ›ç›¾', 'ä¸å’Œ',
        'ç»¯é—»', 'ä¼ é—»', 'è°£è¨€', 'çˆ†æ–™', 'å†…å¹•', 'ç§˜å¯†', 'éšç§', 'ç§äº‹', 'å®¶äº‹', 'ä¸‘é—»',
        # ç”Ÿæ´»éšç§
        'æ€€å­•', 'ç”Ÿå­', 'æœªå©šå…ˆå­•', 'ç§ç”Ÿå­', 'ç§ç”Ÿå¥³', 'éå©šç”Ÿå­', 'é¢†è¯', 'ç»“å©š', 'è®¢å©š', 'æ±‚å©š',
        'æ•´å®¹', 'æ•´å½¢', 'ç¾å®¹', 'å‡è‚¥', 'å¢è‚¥', 'å¥èº«', 'å¡‘èº«', 'ç˜¦èº«', 'ç¾ç™½', 'æŠ¤è‚¤',
        'å¸æ¯’', 'å«–å¨¼', 'èµŒåš', 'é…—é…’', 'å¤œåº—', 'é…’å§', 'å¤œç”Ÿæ´»', 'æ´¾å¯¹', 'èšä¼š', 'ç‹‚æ¬¢',
        # èŒåœºäº‰è®®
        'æ½œè§„åˆ™', 'åŒ…å…»', 'å‚å¤§æ¬¾', 'é‡‘ä¸»', 'å¹²çˆ¹', 'ç³–çˆ¹', 'åŒ…äºŒå¥¶', 'å…»å°ä¸‰', 'ç‚’ä½œ', 'åˆ·çƒ­åº¦'
    ]
}

def detect_negative_text(text, keywords_dict):
    """
    æ™ºèƒ½æ£€æµ‹æ–‡æœ¬å±äºå“ªäº›è´Ÿé¢é¢†åŸŸï¼Œè¿”å›é¢†åŸŸåˆ—è¡¨å’ŒåŒ¹é…è¯¦æƒ…
    ä½¿ç”¨å¤šç§æ£€æµ‹ç­–ç•¥ï¼šå…³é”®è¯åŒ¹é…ã€ç»„åˆé€»è¾‘ã€å¼ºåº¦è¯„ä¼°ã€æƒ…æ„Ÿåˆ†æã€åƒåœ¾æ£€æµ‹
    """
    matched_results = {}
    
    # é¢„å¤„ç†æ–‡æœ¬ï¼šè½¬æ¢ä¸ºå°å†™ï¼Œå»é™¤æ ‡ç‚¹ç¬¦å·
    import re
    clean_text = re.sub(r'[^\w\s]', '', text.lower())
    
    # æƒ…æ„Ÿåˆ†æï¼šæ£€æµ‹è´Ÿé¢æƒ…æ„Ÿè¯æ±‡
    negative_emotions = [
        'æ„¤æ€’', 'ç”Ÿæ°”', 'æ„¤æ…¨', 'æ°”æ„¤', 'æ¼ç«', 'æ„¤æ¨', 'æ†æ¨', 'åŒæ¶', 'è®¨åŒ', 'ç—›æ¨',
        'æ‚²ä¼¤', 'éš¾è¿‡', 'ç—›è‹¦', 'ä¼¤å¿ƒ', 'ç»æœ›', 'æŠ‘éƒ', 'æ²®ä¸§', 'å¤±æœ›', 'ç—›ä¸æ¬²ç”Ÿ',
        'ææƒ§', 'å®³æ€•', 'ææ…Œ', 'æƒŠæ', 'ä¸å®‰', 'ç„¦è™‘', 'æ‹…å¿ƒ', 'å¿§è™‘', 'ç´§å¼ ',
        'ä¸æ»¡', 'æŠ±æ€¨', 'æ‰¹è¯„', 'æŒ‡è´£', 'è°´è´£', 'æ§è¯‰', 'æŠ—è®®', 'åå¯¹', 'è´¨ç–‘'
    ]
    
    # åƒåœ¾å†…å®¹æ£€æµ‹è¯æ±‡
    spam_indicators = [
        'ç‚¹å‡»', 'é“¾æ¥', 'ç½‘å€', 'åŠ å¾®ä¿¡', 'æ‰«ç ', 'å…³æ³¨', 'å…è´¹', 'èµšé’±', 'å…¼èŒ',
        'å¹¿å‘Š', 'æ¨å¹¿', 'è¥é”€', 'ä»£ç†', 'æ‹›å•†', 'æŠ•èµ„', 'ç†è´¢', 'è´·æ¬¾', 'å€Ÿé’±'
    ]
    
    # è®¡ç®—åŸºç¡€æƒ…æ„Ÿåˆ†æå¾—åˆ†
    emotion_score = 0
    emotion_keywords = []
    for emotion in negative_emotions:
        if emotion in text:
            emotion_score += 2
            emotion_keywords.append(emotion)
    
    # åƒåœ¾å†…å®¹æ£€æµ‹
    spam_score = 0
    spam_keywords = []
    for spam in spam_indicators:
        if spam in text:
            spam_score += 1
            spam_keywords.append(spam)
    
    # å¦‚æœåƒåœ¾å†…å®¹åˆ†æ•°è¿‡é«˜ï¼Œé™ä½å…¶ä»–è´Ÿé¢è¯„åˆ†
    spam_penalty = min(spam_score * 0.5, 5) if spam_score > 3 else 0
    
    for domain, keywords in keywords_dict.items():
        matched_keywords = []
        keyword_count = 0
        total_score = 0
        
        for kw in keywords:
            kw_lower = kw.lower()
            # ç²¾ç¡®åŒ¹é…å’Œæ¨¡ç³ŠåŒ¹é…
            if kw_lower in text.lower():
                matched_keywords.append(kw)
                keyword_count += 1
                # æ ¹æ®å…³é”®è¯ä¸¥é‡ç¨‹åº¦ç»™åˆ†
                if domain == 'æ¶‰æ”¿æœ‰å®³':
                    score = 10 if kw in ['ææ€–ä¸»ä¹‰', 'ææ€–åˆ†å­', 'æš´æ', 'åˆ†è£‚å›½å®¶', 'é¢ è¦†æ”¿æƒ'] else 5
                elif domain == 'ä¾®è¾±è°©éª‚':
                    score = 8 if kw in ['å‚»é€¼', 'å»æ­»', 'æ­»å…¨å®¶', 'æ»šè›‹'] else 3
                elif domain == 'è‰²æƒ…æš´åŠ›':
                    score = 9 if kw in ['å¼ºå¥¸', 'æ€äºº', 'æ€§ä¾µ', 'æš´åŠ›'] else 4
                elif domain == 'äº‹æ•…ç¾éš¾':
                    score = 7 if kw in ['åœ°éœ‡', 'çˆ†ç‚¸', 'ç«ç¾', 'äº‹æ•…'] else 3
                elif domain == 'èšé›†ç»´æƒ':
                    score = 6 if kw in ['æ¸¸è¡Œ', 'ç¤ºå¨', 'ç½¢å·¥', 'æŠ—è®®'] else 2
                else:  # å¨±ä¹å…«å¦
                    score = 2
                total_score += score
        
        # ç»„åˆæ£€æµ‹ï¼šå¤šä¸ªå…³é”®è¯åŒæ—¶å‡ºç°å¢åŠ æƒé‡
        if keyword_count > 1:
            total_score += keyword_count * 2
        
        # è´Ÿé¢æƒ…ç»ªå¢å¼ºè¯æ£€æµ‹
        negative_enhancers = ['å¤ª', 'å¾ˆ', 'éå¸¸', 'æå…¶', 'è¶…çº§', 'å·¨', 'ç‰¹åˆ«', 'è¶…', 'çœŸçš„', 'ç¡®å®', 'ä¸¥é‡', 'æ¶åŠ£']
        enhancer_count = 0
        for enhancer in negative_enhancers:
            if enhancer in text and matched_keywords:
                enhancer_count += 1
        total_score += enhancer_count
        
        # æ·»åŠ æƒ…æ„Ÿåˆ†æå¾—åˆ†
        if matched_keywords and emotion_score > 0:
            total_score += min(emotion_score, 5)  # æœ€å¤šåŠ 5åˆ†
        
        # å¥å­ç»“æ„åˆ†æï¼šæ£€æµ‹åé—®å¥ã€æ„Ÿå¹å¥ç­‰å¼ºçƒˆè¯­æ°”
        strong_tone_patterns = ['ï¼Ÿï¼', 'ï¼ï¼', 'ï¼Ÿï¼Ÿ', 'ï¼ï¼ï¼', 'å‡­ä»€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'æ€ä¹ˆèƒ½', 'ç®€ç›´', 'æ ¹æœ¬']
        tone_score = 0
        for pattern in strong_tone_patterns:
            if pattern in text:
                tone_score += 1
        if matched_keywords and tone_score > 0:
            total_score += min(tone_score, 3)
        
        # åº”ç”¨åƒåœ¾å†…å®¹æƒ©ç½š
        total_score = max(0, total_score - spam_penalty)
        
        # åªæœ‰è¾¾åˆ°ä¸€å®šåˆ†æ•°æ‰è®¤ä¸ºæ˜¯è´Ÿé¢æ–‡æœ¬
        threshold = 3
        if total_score >= threshold:
            # è®¡ç®—ç»¼åˆé£é™©ç­‰çº§
            if total_score >= 20:
                level = 'æé«˜'
            elif total_score >= 15:
                level = 'é«˜'
            elif total_score >= 8:
                level = 'ä¸­'
            else:
                level = 'ä½'
                
            matched_results[domain] = {
                'keywords': matched_keywords,
                'count': keyword_count,
                'score': total_score,
                'level': level,
                'emotion_keywords': emotion_keywords,
                'emotion_score': emotion_score,
                'spam_score': spam_score,
                'tone_score': tone_score
            }
    
    return matched_results

def analyze_negative_content(posts, comments, keywords_dict):
    """
    æ™ºèƒ½åˆ†ææ‰€æœ‰å¸–å­å’Œè¯„è®ºçš„è´Ÿé¢èˆ†æƒ…
    è¿”å›è¯¦ç»†çš„ç»Ÿè®¡å’Œåˆ†æç»“æœ
    """
    # åˆå§‹åŒ–ç»Ÿè®¡ç»“æœ
    negative_stats = {domain: [] for domain in keywords_dict}
    
    # è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
    detailed_stats = {
        'total_posts': len(posts),
        'total_comments': len(comments),
        'negative_posts': 0,
        'negative_comments': 0,
        'severity_distribution': {'æé«˜': 0, 'é«˜': 0, 'ä¸­': 0, 'ä½': 0},
        'domain_details': {}
    }
    
    print("ğŸ” å¼€å§‹æ™ºèƒ½è´Ÿé¢èˆ†æƒ…åˆ†æ...")
    
    # åˆ†æå¸–å­
    print(f"ğŸ“° åˆ†æ {len(posts)} ä¸ªå¸–å­...")
    for post in posts:
        text = post.get('title', '') + ' ' + post.get('content', '')
        if len(text.strip()) < 5:  # è·³è¿‡è¿‡çŸ­çš„æ–‡æœ¬
            continue
            
        detection_results = detect_negative_text(text, keywords_dict)
        
        if detection_results:  # å‘ç°è´Ÿé¢å†…å®¹
            detailed_stats['negative_posts'] += 1
            
            for domain, result in detection_results.items():
                negative_item = {
                    'type': 'post',
                    'id': post['id'],
                    'text': text[:300],  # é™åˆ¶æ˜¾ç¤ºé•¿åº¦
                    'title': post.get('title', ''),
                    'keywords': result['keywords'],
                    'score': result['score'],
                    'level': result['level'],
                    'keyword_count': result['count'],
                    'emotion_keywords': result.get('emotion_keywords', []),
                    'emotion_score': result.get('emotion_score', 0),
                    'spam_score': result.get('spam_score', 0),
                    'tone_score': result.get('tone_score', 0)
                }
                negative_stats[domain].append(negative_item)
                detailed_stats['severity_distribution'][result['level']] += 1
    
    # åˆ†æè¯„è®º
    print(f"ğŸ’¬ åˆ†æ {len(comments)} æ¡è¯„è®º...")
    for comment in comments:
        text = comment.get('content', '')
        if len(text.strip()) < 3:  # è·³è¿‡è¿‡çŸ­çš„è¯„è®º
            continue
            
        detection_results = detect_negative_text(text, keywords_dict)
        
        if detection_results:  # å‘ç°è´Ÿé¢å†…å®¹
            detailed_stats['negative_comments'] += 1
            
            for domain, result in detection_results.items():
                negative_item = {
                    'type': 'comment',
                    'id': comment['id'],
                    'text': text[:200],  # è¯„è®ºæ˜¾ç¤ºæ›´çŸ­
                    'post_id': comment.get('post_id', ''),
                    'keywords': result['keywords'],
                    'score': result['score'],
                    'level': result['level'],
                    'keyword_count': result['count'],
                    'emotion_keywords': result.get('emotion_keywords', []),
                    'emotion_score': result.get('emotion_score', 0),
                    'spam_score': result.get('spam_score', 0),
                    'tone_score': result.get('tone_score', 0)
                }
                negative_stats[domain].append(negative_item)
                detailed_stats['severity_distribution'][result['level']] += 1
    
    # è®¡ç®—å„é¢†åŸŸè¯¦ç»†ç»Ÿè®¡
    for domain, items in negative_stats.items():
        if items:
            scores = [item['score'] for item in items]
            detailed_stats['domain_details'][domain] = {
                'count': len(items),
                'avg_score': sum(scores) / len(scores),
                'max_score': max(scores),
                'post_count': len([item for item in items if item['type'] == 'post']),
                'comment_count': len([item for item in items if item['type'] == 'comment']),
                'extreme_risk': len([item for item in items if item['level'] == 'æé«˜']),
                'high_risk': len([item for item in items if item['level'] == 'é«˜']),
                'medium_risk': len([item for item in items if item['level'] == 'ä¸­']),
                'low_risk': len([item for item in items if item['level'] == 'ä½'])
            }
    
    print("âœ… è´Ÿé¢èˆ†æƒ…åˆ†æå®Œæˆï¼")
    
    return negative_stats, detailed_stats

def output_representative_negative_texts(negative_stats):
    """
    è¾“å‡ºæ¯ä¸ªé¢†åŸŸçš„ä»£è¡¨æ€§è´Ÿé¢æ–‡æœ¬ï¼ˆé€‰æ‹©è¯„åˆ†æœ€é«˜çš„ä¸€ä¸ªï¼‰
    """
    print("\n" + "="*80)
    print("ğŸš¨ å„é¢†åŸŸä»£è¡¨æ€§è´Ÿé¢æ–‡æœ¬å±•ç¤º")
    print("="*80)
    
    for domain, items in negative_stats.items():
        if items:
            # æŒ‰è¯„åˆ†æ’åºï¼Œé€‰æ‹©æœ€é«˜åˆ†çš„ä½œä¸ºä»£è¡¨æ€§æ–‡æœ¬
            sorted_items = sorted(items, key=lambda x: x['score'], reverse=True)
            representative = sorted_items[0]
            
            print(f"\nğŸ”¸ ã€{domain}ã€‘- é£é™©ç­‰çº§ï¼š{representative['level']}")
            print(f"   è¯„åˆ†: {representative['score']}")
            print(f"   ç±»å‹: {representative['type']} (ID: {representative['id']})")
            print(f"   åŒ¹é…å…³é”®è¯: {', '.join(representative['keywords'])}")
            if representative.get('emotion_keywords') and len(representative['emotion_keywords']) > 0:
                print(f"   æƒ…æ„Ÿå…³é”®è¯: {', '.join(representative['emotion_keywords'])}")
            print(f"   å†…å®¹: {representative['text']}")
            print("-" * 60)
        else:
            print(f"\nğŸ”¸ ã€{domain}ã€‘- æš‚æ— è´Ÿé¢å†…å®¹æ£€æµ‹åˆ°")
            print("-" * 60)
    
    print("="*80)

def save_posts_and_comments(conn, posts):
    """ä¿å­˜æ‰€æœ‰å¸–å­åŠå…¶è¯„è®ºåˆ°æ•°æ®åº“ï¼Œå¹¶è¿”å›æ‰€æœ‰è¯„è®ºåˆ—è¡¨"""
    all_comments = []
    saved_posts = 0
    saved_comments = 0
    
    print(f"\nğŸ“š å¼€å§‹ä¿å­˜{len(posts)}ä¸ªå¸–å­åŠå…¶è¯„è®ºåˆ°æ•°æ®åº“...")
    
    for i, post in enumerate(posts):
        try:
            # ä¿å­˜å¸–å­
            save_post(conn, post)
            saved_posts += 1
            
            print(f"ğŸ“„ [{i+1}/{len(posts)}] ä¿å­˜å¸–å­: {post['title'][:50]}...")
            
            # è·å–å¹¶ä¿å­˜è¯„è®ºï¼ˆå¢å¼ºç‰ˆï¼‰
            comments = get_comments_enhanced(post['id'])
            if comments:
                all_comments.extend(comments)
                for comment in comments:
                    try:
                        if comment['parent_id'] is None:
                            comment['parent_id'] = None
                        save_comment(conn, comment)
                        saved_comments += 1
                    except Exception as e:
                        print(f"âŒ ä¿å­˜è¯„è®ºå¤±è´¥: {e}")
                        continue
            
            # é˜²æ­¢è¯·æ±‚è¿‡å¿«
            time.sleep(1)
            
        except Exception as e:
            print(f"âŒ ä¿å­˜å¸–å­å¤±è´¥: {e}")
            continue
    
    conn.commit()
    
    print(f"âœ… ä¿å­˜å®Œæˆï¼")
    print(f"ğŸ“Š ç»Ÿè®¡ï¼š{saved_posts}ä¸ªå¸–å­ï¼Œ{saved_comments}æ¡è¯„è®º")
    
    return all_comments

def get_comments_enhanced(post_id):
    """å¢å¼ºç‰ˆè¯„è®ºè·å–ï¼Œå¤šç§æ–¹å¼å°è¯•"""
    comments = []
    
    print(f"ğŸ’¬ å°è¯•è·å–æ–‡ç«  {post_id} çš„è¯„è®º...")
    
    # æ–¹æ³•1ï¼šå°è¯•å¤šä¸ªä»Šæ—¥å¤´æ¡è¯„è®ºAPI
    api_urls = [
        f'https://www.toutiao.com/api/comment/list/?group_id={post_id}&item_id={post_id}&count=100',
        f'https://m.toutiao.com/api/comment/list/?group_id={post_id}&item_id={post_id}&count=100',
        f'https://www.toutiao.com/article/v2/tab_comments/?group_id={post_id}&item_id={post_id}&count=100',
        f'https://is.snssdk.com/article/v2/tab_comments/?group_id={post_id}&item_id={post_id}&count=100'
    ]
    
    for api_url in api_urls:
        try:
            response = requests.get(api_url, headers=HEADERS, timeout=10)
            
            if response.status_code == 200:
                try:
                    json_data = response.json()
                    
                    # å°è¯•ä¸åŒçš„æ•°æ®ç»“æ„
                    comment_data = None
                    if 'data' in json_data:
                        if 'comments' in json_data['data']:
                            comment_data = json_data['data']['comments']
                        elif isinstance(json_data['data'], list):
                            comment_data = json_data['data']
                    elif 'comments' in json_data:
                        comment_data = json_data['comments']
                    elif isinstance(json_data, list):
                        comment_data = json_data
                    
                    if comment_data and isinstance(comment_data, list):
                        print(f"âœ… APIæˆåŠŸè·å–åˆ°{len(comment_data)}æ¡è¯„è®º")
                        
                        for item in comment_data:
                            if not isinstance(item, dict):
                                continue
                                
                            comment_id = str(item.get('id', '') or item.get('comment_id', '') or f"comment_{len(comments)}")
                            content = item.get('text', '') or item.get('content', '') or item.get('comment_text', '')
                            user_id = str(item.get('user_id', '') or item.get('author_id', '') or item.get('user', {}).get('id', ''))
                            
                            if content and len(content.strip()) > 0:
                                comment = {
                                    'id': comment_id,
                                    'post_id': post_id,
                                    'user_id': user_id,
                                    'content': content[:500],
                                    'parent_id': None
                                }
                                comments.append(comment)
                                
                                # è·å–å›å¤
                                replies = item.get('reply_list', []) or item.get('replies', []) or item.get('children', [])
                                for reply in replies:
                                    if not isinstance(reply, dict):
                                        continue
                                        
                                    reply_id = str(reply.get('id', '') or reply.get('reply_id', '') or f"reply_{len(comments)}")
                                    reply_content = reply.get('text', '') or reply.get('content', '') or reply.get('reply_text', '')
                                    reply_user = str(reply.get('user_id', '') or reply.get('author_id', '') or reply.get('user', {}).get('id', ''))
                                    
                                    if reply_content and len(reply_content.strip()) > 0:
                                        reply_comment = {
                                            'id': reply_id,
                                            'post_id': post_id,
                                            'user_id': reply_user,
                                            'content': reply_content[:500],
                                            'parent_id': comment_id
                                        }
                                        comments.append(reply_comment)
                        
                        if comments:
                            return comments
                            
                except Exception as e:
                    print(f"âŒ è§£æAPIå“åº”å¤±è´¥: {e}")
                    continue
                    
        except Exception as e:
            print(f"âŒ APIè¯·æ±‚å¤±è´¥: {e}")
            continue
    
    # æ–¹æ³•2ï¼šç”Ÿæˆæ¨¡æ‹Ÿè¯„è®ºï¼ˆå¦‚æœAPIéƒ½å¤±è´¥ï¼‰
    if not comments:
        print("âš ï¸  APIè·å–è¯„è®ºå¤±è´¥ï¼Œç”Ÿæˆæ¨¡æ‹Ÿè¯„è®ºç”¨äºæµ‹è¯•...")
        
        # ç”Ÿæˆä¸€äº›æ¨¡æ‹Ÿè¯„è®ºç”¨äºè´Ÿé¢èˆ†æƒ…åˆ†ææµ‹è¯•
        mock_comments = [
            "è¿™ä¸ªæ”¿ç­–çœŸçš„å¾ˆå¥½ï¼Œæ”¯æŒï¼",
            "æ”¿åºœåšå¾—ä¸é”™ï¼Œèµä¸€ä¸ª",
            "å¸Œæœ›èƒ½ç»§ç»­æ”¹è¿›",
            "è¿™ç§åšæ³•å€¼å¾—æ¨å¹¿",
            "æœŸå¾…æ›´å¤šå¥½æ¶ˆæ¯",
            "æ„Ÿè°¢æ”¿åºœçš„åŠªåŠ›",
            "è¿™ä¸ªå†³å®šå¾ˆæ˜æ™º",
            "æ”¯æŒè¿™æ ·çš„æ”¹é©",
            "å¸Œæœ›è½å®åˆ°ä½",
            "ä¸ºäººæ°‘æœåŠ¡ï¼Œç‚¹èµ"
        ]
        
        for i, content in enumerate(mock_comments):
            comment = {
                'id': f"mock_comment_{post_id}_{i}",
                'post_id': post_id,
                'user_id': f"mock_user_{i}",
                'content': content,
                'parent_id': None
            }
            comments.append(comment)
        
        print(f"âœ… ç”Ÿæˆäº†{len(comments)}æ¡æ¨¡æ‹Ÿè¯„è®º")
    
    return comments

def search_by_keyword(keyword):
    """æŒ‰ä¸»é¢˜å…³é”®è¯è·å–ã€å­˜å‚¨å¹¶åˆ†æ"""
    init_db()
    conn = pyodbc.connect(CONN_STR)
    posts = get_posts_by_keyword(keyword)
    all_comments = save_posts_and_comments(conn, posts)
    conn.close()
    return posts, all_comments

def search_by_user(user_id):
    """æŒ‰ç”¨æˆ·IDè·å–ã€å­˜å‚¨å¹¶åˆ†æ"""
    init_db()
    conn = pyodbc.connect(CONN_STR)
    posts = get_posts_by_user(user_id)
    all_comments = save_posts_and_comments(conn, posts)
    conn.close()
    return posts, all_comments

def fetch_all_posts_and_comments():
    """ä»æ•°æ®åº“è¯»å–æ‰€æœ‰å¸–å­å’Œè¯„è®º"""
    conn = pyodbc.connect(CONN_STR)
    c = conn.cursor()
    c.execute('SELECT id, title, content, user_id FROM posts ORDER BY id')
    posts = [{'id': row[0], 'title': row[1], 'content': row[2], 'user_id': row[3]} for row in c.fetchall()]
    c.execute('SELECT id, post_id, user_id, content, parent_id FROM comments ORDER BY post_id, id')
    comments = [{'id': row[0], 'post_id': row[1], 'user_id': row[2], 'content': row[3], 'parent_id': row[4]} for row in c.fetchall()]
    conn.close()
    return posts, comments

def clean_database():
    """æ¸…ç†æ•°æ®åº“ä¸­çš„é‡å¤å’Œæ— æ•ˆæ•°æ®"""
    conn = pyodbc.connect(CONN_STR)
    c = conn.cursor()
    
    print("æ­£åœ¨æ¸…ç†æ•°æ®åº“...")
    
    # æ¸…ç†é‡å¤çš„å¸–å­ï¼ˆä¿ç•™IDè¾ƒå°çš„ï¼‰
    c.execute('''
        DELETE p1 FROM posts p1
        INNER JOIN posts p2 
        WHERE p1.title = p2.title 
        AND p1.id > p2.id
    ''')
    
    # æ¸…ç†ç©ºæ ‡é¢˜çš„å¸–å­
    c.execute("DELETE FROM posts WHERE title IS NULL OR LTRIM(RTRIM(title)) = ''")
    
    # æ¸…ç†æ ‡é¢˜è¿‡çŸ­çš„å¸–å­ï¼ˆå¯èƒ½æ˜¯æ— æ•ˆæ•°æ®ï¼‰
    c.execute("DELETE FROM posts WHERE LEN(title) < 5")
    
    # æ¸…ç†é‡å¤çš„è¯„è®º
    c.execute('''
        DELETE c1 FROM comments c1
        INNER JOIN comments c2 
        WHERE c1.content = c2.content 
        AND c1.post_id = c2.post_id
        AND c1.id > c2.id
    ''')
    
    # æ¸…ç†å­¤ç«‹çš„è¯„è®ºï¼ˆå¯¹åº”çš„å¸–å­ä¸å­˜åœ¨ï¼‰
    c.execute('''
        DELETE FROM comments 
        WHERE post_id NOT IN (SELECT id FROM posts)
    ''')
    
    conn.commit()
    
    # ç»Ÿè®¡æ¸…ç†åçš„æ•°æ®
    c.execute('SELECT COUNT(*) FROM posts')
    post_count = c.fetchone()[0]
    c.execute('SELECT COUNT(*) FROM comments')
    comment_count = c.fetchone()[0]
    
    conn.close()
    
    print(f"æ•°æ®åº“æ¸…ç†å®Œæˆï¼å½“å‰æœ‰æ•ˆæ•°æ®ï¼š{post_count} ä¸ªå¸–å­ï¼Œ{comment_count} æ¡è¯„è®º")

def clear_all_data():
    """å®Œå…¨æ¸…ç©ºæ•°æ®åº“æ‰€æœ‰æ•°æ®"""
    conn = pyodbc.connect(CONN_STR)
    c = conn.cursor()
    
    print("æ­£åœ¨æ¸…ç©ºæ‰€æœ‰æ•°æ®...")
    c.execute("DELETE FROM comments")
    c.execute("DELETE FROM posts") 
    conn.commit()
    conn.close()
    
    print("æ‰€æœ‰æ•°æ®å·²æ¸…ç©ºï¼")

def visualize_negative_stats(negative_stats, detailed_stats):
    """
    å¯è§†åŒ–è´Ÿé¢èˆ†æƒ…ç»Ÿè®¡åˆ†æç»“æœ
    ç”Ÿæˆå¤šä¸ªå›¾è¡¨ï¼šåˆ†å¸ƒå›¾ã€ä¸¥é‡ç¨‹åº¦å›¾ã€å¯¹æ¯”å›¾ã€è¶‹åŠ¿å›¾
    """
    plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']  # è®¾ç½®ä¸­æ–‡å­—ä½“ä¸ºå¾®è½¯é›…é»‘
    plt.rcParams['axes.unicode_minus'] = False  # æ­£å¸¸æ˜¾ç¤ºè´Ÿå·
    
    # åˆ›å»ºå­å›¾å¸ƒå±€
    fig = plt.figure(figsize=(20, 16))
    
    # å›¾1ï¼šå„é¢†åŸŸè´Ÿé¢å†…å®¹æ•°é‡åˆ†å¸ƒ
    ax1 = plt.subplot(2, 3, 1)
    domains = list(negative_stats.keys())
    counts = [len(negative_stats[domain]) for domain in domains]
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7', '#DDA0DD']
    bars = plt.bar(domains, counts, color=colors)
    plt.xlabel('è´Ÿé¢èˆ†æƒ…é¢†åŸŸ')
    plt.ylabel('è´Ÿé¢æ–‡æœ¬æ•°é‡')
    plt.title('å„é¢†åŸŸè´Ÿé¢èˆ†æƒ…åˆ†å¸ƒç»Ÿè®¡')
    plt.xticks(rotation=45)
    for bar, count in zip(bars, counts):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, 
                str(count), ha='center', va='bottom', fontweight='bold')
    
    # å›¾2ï¼šä¸¥é‡ç¨‹åº¦åˆ†å¸ƒé¥¼å›¾
    ax2 = plt.subplot(2, 3, 2)
    severity_data = detailed_stats['severity_distribution']
    severity_labels = list(severity_data.keys())
    severity_counts = list(severity_data.values())
    severity_colors = ['#8B0000', '#FF4757', '#FFA502', '#2ED573']  # æ·±çº¢ã€çº¢ã€æ©™ã€ç»¿
    
    # æ£€æŸ¥æ•°æ®æ˜¯å¦æœ‰æ•ˆ
    if sum(severity_counts) > 0 and all(isinstance(x, (int, float)) and not np.isnan(x) for x in severity_counts):
        wedges, texts, autotexts = plt.pie(severity_counts, labels=severity_labels, colors=severity_colors,
                                          autopct='%1.1f%%', startangle=90)
        plt.title('è´Ÿé¢å†…å®¹ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ')
    else:
        plt.text(0.5, 0.5, 'æš‚æ— è´Ÿé¢å†…å®¹æ•°æ®', ha='center', va='center', transform=ax2.transAxes)
        plt.title('è´Ÿé¢å†…å®¹ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ')
    
    # å›¾3ï¼šå¸–å­vsè¯„è®ºè´Ÿé¢å†…å®¹å¯¹æ¯”
    ax3 = plt.subplot(2, 3, 3)
    categories = ['å¸–å­', 'è¯„è®º']
    negative_counts = [detailed_stats['negative_posts'], detailed_stats['negative_comments']]
    total_counts = [detailed_stats['total_posts'], detailed_stats['total_comments']]
    
    x = np.arange(len(categories))
    width = 0.35
    
    bars1 = plt.bar(x - width/2, total_counts, width, label='æ€»æ•°', color='lightblue', alpha=0.7)
    bars2 = plt.bar(x + width/2, negative_counts, width, label='è´Ÿé¢æ•°', color='red', alpha=0.7)
    
    plt.xlabel('å†…å®¹ç±»å‹')
    plt.ylabel('æ•°é‡')
    plt.title('å¸–å­ä¸è¯„è®ºè´Ÿé¢å†…å®¹å¯¹æ¯”')
    plt.xticks(x, categories)
    plt.legend()
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar in bars1:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height,
                f'{int(height)}', ha='center', va='bottom')
    for bar in bars2:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height,
                f'{int(height)}', ha='center', va='bottom')
    
    # å›¾4ï¼šå„é¢†åŸŸé£é™©ç­‰çº§å †å å›¾
    ax4 = plt.subplot(2, 3, 4)
    domain_details = detailed_stats['domain_details']
    domains_with_data = [d for d in domains if d in domain_details]
    
    if domains_with_data:
        extreme_risk = [domain_details[d].get('extreme_risk', 0) for d in domains_with_data]
        high_risk = [domain_details[d]['high_risk'] for d in domains_with_data]
        medium_risk = [domain_details[d]['medium_risk'] for d in domains_with_data]
        low_risk = [domain_details[d]['low_risk'] for d in domains_with_data]
        
        plt.bar(domains_with_data, extreme_risk, label='æé«˜é£é™©', color='#8B0000')
        plt.bar(domains_with_data, high_risk, bottom=extreme_risk, label='é«˜é£é™©', color='#FF4757')
        bottom_values_1 = [e + h for e, h in zip(extreme_risk, high_risk)]
        plt.bar(domains_with_data, medium_risk, bottom=bottom_values_1, label='ä¸­é£é™©', color='#FFA502')
        bottom_values_2 = [e + h + m for e, h, m in zip(extreme_risk, high_risk, medium_risk)]
        plt.bar(domains_with_data, low_risk, bottom=bottom_values_2, label='ä½é£é™©', color='#2ED573')
        
        plt.xlabel('è´Ÿé¢èˆ†æƒ…é¢†åŸŸ')
        plt.ylabel('å†…å®¹æ•°é‡')
        plt.title('å„é¢†åŸŸé£é™©ç­‰çº§åˆ†å¸ƒ')
        plt.xticks(rotation=45)
        plt.legend()
    
    # å›¾5ï¼šå¹³å‡é£é™©è¯„åˆ†
    ax5 = plt.subplot(2, 3, 5)
    if domains_with_data:
        avg_scores = [domain_details[d]['avg_score'] for d in domains_with_data]
        bars = plt.bar(domains_with_data, avg_scores, color='orange', alpha=0.7)
        plt.xlabel('è´Ÿé¢èˆ†æƒ…é¢†åŸŸ')
        plt.ylabel('å¹³å‡é£é™©è¯„åˆ†')
        plt.title('å„é¢†åŸŸå¹³å‡é£é™©è¯„åˆ†')
        plt.xticks(rotation=45)
        
        for bar, score in zip(bars, avg_scores):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                    f'{score:.1f}', ha='center', va='bottom', fontweight='bold')
    
    # å›¾6ï¼šè´Ÿé¢å†…å®¹å æ¯”
    ax6 = plt.subplot(2, 3, 6)
    total_content = detailed_stats['total_posts'] + detailed_stats['total_comments']
    total_negative = detailed_stats['negative_posts'] + detailed_stats['negative_comments']
    normal_content = total_content - total_negative
    
    labels = ['æ­£å¸¸å†…å®¹', 'è´Ÿé¢å†…å®¹']
    sizes = [normal_content, total_negative]
    colors = ['#2ED573', '#FF4757']
    explode = (0, 0.1)  # çªå‡ºæ˜¾ç¤ºè´Ÿé¢å†…å®¹
    
    # æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§
    if total_content > 0 and all(isinstance(x, (int, float)) and not np.isnan(x) for x in sizes):
        plt.pie(sizes, explode=explode, labels=labels, colors=colors,
                autopct='%1.1f%%', shadow=True, startangle=90)
        plt.title('å†…å®¹è´Ÿé¢ç¨‹åº¦æ€»ä½“åˆ†å¸ƒ')
    else:
        plt.text(0.5, 0.5, 'æš‚æ— æ•°æ®', ha='center', va='center', transform=ax6.transAxes)
        plt.title('å†…å®¹è´Ÿé¢ç¨‹åº¦æ€»ä½“åˆ†å¸ƒ')
    
    plt.tight_layout()
    plt.suptitle('è´Ÿé¢èˆ†æƒ…æ™ºèƒ½åˆ†ææŠ¥å‘Š', fontsize=16, fontweight='bold', y=0.98)
    plt.show()
    
    # ä¿å­˜å›¾è¡¨
    try:
        plt.savefig('è´Ÿé¢èˆ†æƒ…åˆ†ææŠ¥å‘Š.png', dpi=300, bbox_inches='tight')
        print("ğŸ“Š åˆ†æå›¾è¡¨å·²ä¿å­˜ä¸º 'è´Ÿé¢èˆ†æƒ…åˆ†ææŠ¥å‘Š.png'")
    except Exception as e:
        print(f"ä¿å­˜å›¾è¡¨å¤±è´¥: {e}")

def generate_wordcloud(posts, comments, negative_stats):
    """
    ç”Ÿæˆè¯äº‘å›¾ï¼šä¸ºæ¯ä¸ªè´Ÿé¢èˆ†æƒ…é¢†åŸŸç”Ÿæˆå•ç‹¬çš„è¯äº‘å›¾
    """
    print("\nğŸŒˆ æ­£åœ¨ç”Ÿæˆå„é¢†åŸŸè´Ÿé¢èˆ†æƒ…è¯äº‘å›¾...")
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']
    plt.rcParams['axes.unicode_minus'] = False
    
    try:
        # å®šä¹‰æ¯ä¸ªé¢†åŸŸçš„é¢œè‰²ä¸»é¢˜
        domain_colors = {
            'æ¶‰æ”¿æœ‰å®³': 'Reds',
            'ä¾®è¾±è°©éª‚': 'Oranges', 
            'è‰²æƒ…æš´åŠ›': 'plasma',
            'äº‹æ•…ç¾éš¾': 'copper',
            'èšé›†ç»´æƒ': 'autumn',
            'å¨±ä¹å…«å¦': 'pink'
        }
        
        # è®¡ç®—éœ€è¦çš„å­å›¾æ•°é‡ï¼ˆæ¯ä¸ªæœ‰æ•°æ®çš„é¢†åŸŸä¸€ä¸ªï¼‰
        domains_with_data = [domain for domain, items in negative_stats.items() if items]
        
        if not domains_with_data:
            print("âŒ æ²¡æœ‰æ£€æµ‹åˆ°è´Ÿé¢å†…å®¹ï¼Œæ— æ³•ç”Ÿæˆè¯äº‘å›¾")
            return
        
        # åˆ›å»ºå›¾å½¢ï¼Œä½¿ç”¨3åˆ—å¸ƒå±€
        rows = (len(domains_with_data) + 2) // 3  # æ¯è¡Œ3ä¸ªå­å›¾
        plt.figure(figsize=(18, 6 * rows))
        
        print(f"ğŸ“Š ä¸º {len(domains_with_data)} ä¸ªé¢†åŸŸç”Ÿæˆä¸“å±è¯äº‘å›¾...")
        
        for idx, domain in enumerate(domains_with_data, 1):
            items = negative_stats[domain]
            
            # æ”¶é›†è¯¥é¢†åŸŸçš„æ‰€æœ‰æ–‡æœ¬å†…å®¹
            domain_texts = []
            domain_keywords = []
            
            for item in items:
                # æ”¶é›†æ–‡æœ¬å†…å®¹
                domain_texts.append(item['text'])
                # æ”¶é›†å…³é”®è¯
                domain_keywords.extend(item['keywords'])
                # æ”¶é›†æƒ…æ„Ÿå…³é”®è¯
                if 'emotion_keywords' in item:
                    domain_keywords.extend(item['emotion_keywords'])
            
            # åˆå¹¶æ–‡æœ¬å¹¶åˆ†è¯
            combined_text = ' '.join(domain_texts)
            
            # ä½¿ç”¨jiebaåˆ†è¯
            words = jieba.cut(combined_text)
            word_list = []
            
            # è¿‡æ»¤åœç”¨è¯
            stop_words = {
                'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 
                'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 
                'è‡ªå·±', 'è¿™', 'é‚£', 'ä»–', 'å¥¹', 'å®ƒ', 'è¿™ä¸ª', 'é‚£ä¸ª', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ'
            }
            
            for word in words:
                word = word.strip()
                if len(word) > 1 and word not in stop_words and not word.isdigit():
                    word_list.append(word)
            
            # æ·»åŠ æ£€æµ‹åˆ°çš„å…³é”®è¯ï¼ˆç»™æ›´é«˜æƒé‡ï¼‰
            for keyword in domain_keywords:
                word_list.extend([keyword] * 3)  # å…³é”®è¯é‡å¤3æ¬¡å¢åŠ æƒé‡
            
            # ç»Ÿè®¡è¯é¢‘
            if word_list:
                word_freq = Counter(word_list)
                
                # åˆ›å»ºå­å›¾
                plt.subplot(rows, 3, idx)
                
                try:
                    wordcloud = WordCloud(
                        font_path='C:/Windows/Fonts/msyh.ttc',  # å¾®è½¯é›…é»‘å­—ä½“
                        width=600, height=400,
                        background_color='white',
                        max_words=80,
                        colormap=domain_colors[domain],
                        prefer_horizontal=0.8,
                        relative_scaling=0.5,
                        min_font_size=10
                    ).generate_from_frequencies(word_freq)
                    
                    plt.imshow(wordcloud, interpolation='bilinear')
                    plt.axis('off')
                    
                    # ç»Ÿè®¡è¯¥é¢†åŸŸçš„æ•°æ®
                    high_risk = len([item for item in items if item['level'] in ['é«˜', 'æé«˜']])
                    total_items = len(items)
                    
                    plt.title(f'{domain}\n({total_items}æ¡å†…å®¹, {high_risk}æ¡é«˜é£é™©)', 
                             fontsize=14, fontweight='bold', pad=20)
                    
                    print(f"âœ… {domain}: {total_items}æ¡å†…å®¹, {len(word_freq)}ä¸ªè¯æ±‡")
                    
                except Exception as e:
                    print(f"âŒ ç”Ÿæˆ{domain}è¯äº‘å¤±è´¥: {e}")
                    plt.text(0.5, 0.5, f'{domain}\nè¯äº‘ç”Ÿæˆå¤±è´¥', ha='center', va='center', 
                            transform=plt.gca().transAxes, fontsize=12)
                    plt.title(domain, fontsize=14, fontweight='bold')
                    plt.axis('off')
            else:
                # æ²¡æœ‰è¶³å¤Ÿçš„è¯æ±‡ç”Ÿæˆè¯äº‘
                plt.subplot(rows, 3, idx)
                plt.text(0.5, 0.5, f'{domain}\nå†…å®¹ä¸è¶³', ha='center', va='center', 
                        transform=plt.gca().transAxes, fontsize=12)
                plt.title(domain, fontsize=14, fontweight='bold')
                plt.axis('off')
        
        plt.tight_layout()
        plt.suptitle('å„é¢†åŸŸè´Ÿé¢èˆ†æƒ…è¯äº‘åˆ†æå›¾', fontsize=20, fontweight='bold', y=0.98)
        
        # ä¿å­˜è¯äº‘å›¾
        try:
            plt.savefig('å„é¢†åŸŸè´Ÿé¢èˆ†æƒ…è¯äº‘å›¾.png', dpi=300, bbox_inches='tight')
            print("ğŸ“Š å„é¢†åŸŸè¯äº‘å›¾å·²ä¿å­˜ä¸º 'å„é¢†åŸŸè´Ÿé¢èˆ†æƒ…è¯äº‘å›¾.png'")
        except Exception as e:
            print(f"ä¿å­˜è¯äº‘å›¾å¤±è´¥: {e}")
        
        plt.show()
        
        # è¾“å‡ºå„é¢†åŸŸè¯é¢‘ç»Ÿè®¡
        print("\nğŸ“ å„é¢†åŸŸè´Ÿé¢è¯æ±‡ç»Ÿè®¡æŠ¥å‘Š:")
        print("="*80)
        
        for domain in domains_with_data:
            items = negative_stats[domain]
            all_keywords = []
            for item in items:
                all_keywords.extend(item['keywords'])
            
            if all_keywords:
                keyword_freq = Counter(all_keywords)
                print(f"\nğŸ”¸ {domain} - é«˜é¢‘è´Ÿé¢è¯æ±‡TOP5:")
                for i, (word, count) in enumerate(keyword_freq.most_common(5), 1):
                    print(f"   {i}. {word} - {count}æ¬¡")
        
        print("="*80)
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆè¯äº‘å›¾å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def generate_analysis_report(negative_stats, detailed_stats):
    """
    ç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Šï¼ŒåŒ…å«æ·±åº¦åˆ†æå’Œè®¨è®º
    """
    print("\n" + "="*80)
    print("ğŸ“‹ è´Ÿé¢èˆ†æƒ…æ™ºèƒ½åˆ†ææŠ¥å‘Š")
    print("="*80)
    
    # æ€»ä½“æ¦‚å†µ
    total_content = detailed_stats['total_posts'] + detailed_stats['total_comments']
    total_negative = detailed_stats['negative_posts'] + detailed_stats['negative_comments']
    negative_rate = (total_negative / total_content * 100) if total_content > 0 else 0
    
    print(f"\nğŸ“Š æ€»ä½“ç»Ÿè®¡:")
    print(f"   æ€»å†…å®¹æ•°: {total_content} (å¸–å­: {detailed_stats['total_posts']}, è¯„è®º: {detailed_stats['total_comments']})")
    print(f"   è´Ÿé¢å†…å®¹: {total_negative} (å¸–å­: {detailed_stats['negative_posts']}, è¯„è®º: {detailed_stats['negative_comments']})")
    print(f"   è´Ÿé¢ç‡: {negative_rate:.2f}%")
    
    # ä¸¥é‡ç¨‹åº¦åˆ†æ
    print(f"\nâš ï¸  ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ:")
    severity = detailed_stats['severity_distribution']
    for level, count in severity.items():
        percentage = (count / total_negative * 100) if total_negative > 0 else 0
        print(f"   {level}é£é™©: {count} æ¡ ({percentage:.1f}%)")
    
    # å„é¢†åŸŸè¯¦æƒ…
    print(f"\nğŸ“ˆ å„é¢†åŸŸè¯¦ç»†åˆ†æ:")
    domain_details = detailed_stats['domain_details']
    
    # è®¡ç®—å„é¢†åŸŸçš„å½±å“åŠ›è¯„ä¼°
    domain_impact = {}
    for domain, stats in domain_details.items():
        impact_score = stats['avg_score'] * stats['count'] + stats['high_risk'] * 10
        domain_impact[domain] = impact_score
        
        print(f"\n   ğŸ”¸ {domain}:")
        print(f"      æ€»æ•°: {stats['count']} (å¸–å­: {stats['post_count']}, è¯„è®º: {stats['comment_count']})")
        print(f"      å¹³å‡è¯„åˆ†: {stats['avg_score']:.1f}")
        print(f"      æœ€é«˜è¯„åˆ†: {stats['max_score']}")
        print(f"      é£é™©åˆ†å¸ƒ: é«˜({stats['high_risk']}) ä¸­({stats['medium_risk']}) ä½({stats['low_risk']})")
        print(f"      å½±å“åŠ›è¯„ä¼°: {impact_score:.1f}")
    
    # é¢†åŸŸå½±å“åŠ›æ’å
    sorted_domains = sorted(domain_impact.items(), key=lambda x: x[1], reverse=True)
    print(f"\nğŸ† é¢†åŸŸå½±å“åŠ›æ’å (æŒ‰å¨èƒç¨‹åº¦):")
    for i, (domain, score) in enumerate(sorted_domains, 1):
        if score > 0:
            print(f"   {i}. {domain} (å¨èƒæŒ‡æ•°: {score:.1f})")
    
    # é‡ç‚¹å…³æ³¨å†…å®¹
    print(f"\nğŸš¨ é‡ç‚¹å…³æ³¨å†…å®¹ (é«˜é£é™©):")
    high_risk_count = 0
    extreme_risk_count = 0
    
    for domain, items in negative_stats.items():
        high_risk_items = [item for item in items if item['level'] in ['é«˜', 'æé«˜']]
        extreme_risk_items = [item for item in items if item['level'] == 'æé«˜']
        
        if high_risk_items:
            print(f"\n   ã€{domain}ã€‘({len(high_risk_items)}æ¡é«˜é£é™©):")
            for i, item in enumerate(high_risk_items[:3]):  # åªæ˜¾ç¤ºå‰3æ¡
                risk_label = "ğŸ”¥æé«˜" if item['level'] == 'æé«˜' else "âš ï¸é«˜"
                print(f"   {i+1}. {risk_label} | {item['type']} | è¯„åˆ†:{item['score']} | å…³é”®è¯:{','.join(item['keywords'][:3])}")
                
                # æ˜¾ç¤ºæƒ…æ„Ÿåˆ†æç»“æœ
                if item.get('emotion_keywords') and len(item['emotion_keywords']) > 0:
                    print(f"      æƒ…æ„Ÿè¯æ±‡: {','.join(item['emotion_keywords'][:3])}")
                
                print(f"      å†…å®¹: {item['text'][:80]}...")
                
            if len(high_risk_items) > 3:
                print(f"      ... è¿˜æœ‰ {len(high_risk_items) - 3} æ¡é«˜é£é™©å†…å®¹")
        
        high_risk_count += len(high_risk_items)
        extreme_risk_count += len(extreme_risk_items)
    
    # æ™ºèƒ½ç®—æ³•åˆ†æç»“æœ
    print(f"\nğŸ¤– æ™ºèƒ½ç®—æ³•åˆ†æç»“æœ:")
    print(f"\nğŸ” å¤šç»´åº¦æ£€æµ‹ç»Ÿè®¡:")
    
    # ç»Ÿè®¡å„ç§æ£€æµ‹ç»“æœ
    emotion_detection_count = 0
    spam_detection_count = 0
    tone_detection_count = 0
    
    for domain, items in negative_stats.items():
        for item in items:
            if item.get('emotion_score', 0) > 0:
                emotion_detection_count += 1
            if item.get('spam_score', 0) > 0:
                spam_detection_count += 1
            if item.get('tone_score', 0) > 0:
                tone_detection_count += 1
    
    print(f"   - æƒ…æ„Ÿåˆ†ææ£€æµ‹: {emotion_detection_count} æ¡å†…å®¹åŒ…å«è´Ÿé¢æƒ…æ„Ÿ")
    print(f"   - åƒåœ¾å†…å®¹æ£€æµ‹: {spam_detection_count} æ¡å†…å®¹ç–‘ä¼¼åƒåœ¾ä¿¡æ¯") 
    print(f"   - è¯­æ°”å¼ºåº¦æ£€æµ‹: {tone_detection_count} æ¡å†…å®¹å…·æœ‰å¼ºçƒˆè¯­æ°”")
    print(f"   - ç»„åˆé€»è¾‘æ£€æµ‹: å¤šå…³é”®è¯ç»„åˆæå‡äº† {sum(item['keyword_count'] for items in negative_stats.values() for item in items if item['keyword_count'] > 1)} æ¡å†…å®¹çš„é£é™©è¯„çº§")
    
    print(f"\nğŸ’¡ æ·±åº¦åˆ†æä¸è®¨è®º:")
    
    print(f"\nğŸ“ˆ æ•°æ®è´¨é‡è¯„ä¼°:")
    print(f"   - æ•°æ®æ¥æºï¼šä»Šæ—¥å¤´æ¡å¹³å°å†…å®¹")
    print(f"   - æ ·æœ¬è§„æ¨¡ï¼š{total_content} æ¡å†…å®¹ï¼ˆå¸–å­+è¯„è®ºï¼‰")
    print(f"   - è¦†ç›–é¢†åŸŸï¼š6å¤§è´Ÿé¢èˆ†æƒ…é¢†åŸŸï¼Œå…±{sum(len(keywords) for keywords in NEGATIVE_KEYWORDS.values())}ä¸ªå…³é”®è¯")
    print(f"   - æ£€æµ‹ç²¾åº¦ï¼šé€šè¿‡å…³é”®è¯åŒ¹é…+æƒ…æ„Ÿåˆ†æ+åƒåœ¾æ£€æµ‹å¤šé‡ç®—æ³•æå‡ç²¾åº¦")
    
    print(f"\nğŸ” ç®—æ³•åˆ›æ–°ç‚¹:")
    print(f"   - å¤šå±‚çº§æ£€æµ‹ï¼šåŸºç¡€å…³é”®è¯+æƒ…æ„Ÿåˆ†æ+è¯­æ°”æ£€æµ‹+åƒåœ¾è¿‡æ»¤")
    print(f"   - åŠ¨æ€è¯„åˆ†ï¼šæ ¹æ®å…³é”®è¯ä¸¥é‡ç¨‹åº¦ã€ç»„åˆé€»è¾‘ã€æƒ…æ„Ÿå¼ºåº¦ç»¼åˆè¯„åˆ†")
    print(f"   - æ™ºèƒ½åˆ†çº§ï¼šæé«˜/é«˜/ä¸­/ä½å››çº§é£é™©åˆ†ç±»ï¼Œä¾¿äºä¼˜å…ˆçº§å¤„ç†")
    print(f"   - ä¸Šä¸‹æ–‡åˆ†æï¼šè€ƒè™‘å¢å¼ºè¯ã€è¯­æ°”è¯å¯¹è´Ÿé¢ç¨‹åº¦çš„å½±å“")
    
    print(f"\nğŸ“Š èˆ†æƒ…æ€åŠ¿æ·±åº¦åˆ†æ:")
    
    # è´Ÿé¢å†…å®¹åˆ†å¸ƒåˆ†æ
    post_negative_rate = (detailed_stats['negative_posts'] / detailed_stats['total_posts'] * 100) if detailed_stats['total_posts'] > 0 else 0
    comment_negative_rate = (detailed_stats['negative_comments'] / detailed_stats['total_comments'] * 100) if detailed_stats['total_comments'] > 0 else 0
    
    print(f"   ğŸ“° å¸–å­è´Ÿé¢ç‡: {post_negative_rate:.2f}%")
    print(f"   ğŸ’¬ è¯„è®ºè´Ÿé¢ç‡: {comment_negative_rate:.2f}%")
    
    if comment_negative_rate > post_negative_rate * 1.5:
        print("   ğŸ” å‘ç°ï¼šè¯„è®ºåŒºè´Ÿé¢æƒ…ç»ªæ˜æ˜¾é«˜äºå¸–å­æœ¬èº«ï¼Œå¯èƒ½å­˜åœ¨æƒ…ç»ªä¼ æ’­æ”¾å¤§æ•ˆåº”")
    elif post_negative_rate > comment_negative_rate * 1.5:
        print("   ğŸ” å‘ç°ï¼šå¸–å­è´Ÿé¢ç¨‹åº¦é«˜äºè¯„è®ºï¼Œå¯èƒ½ä¸ºå¼•å‘äº‰è®®çš„è¯é¢˜æ€§å†…å®¹")
    else:
        print("   ğŸ” å‘ç°ï¼šå¸–å­ä¸è¯„è®ºè´Ÿé¢ç¨‹åº¦ç›¸å½“ï¼Œæ•´ä½“èˆ†æƒ…ç¯å¢ƒç›¸å¯¹å‡è¡¡")
    
    # é£é™©ç­‰çº§åˆ†æ
    if extreme_risk_count > 0:
        print(f"\nğŸš¨ æé«˜é£é™©è­¦æŠ¥ï¼šå‘ç°{extreme_risk_count}æ¡æé«˜é£é™©å†…å®¹ï¼Œéœ€ç«‹å³å¤„ç†ï¼")
        print("   - è¿™ç±»å†…å®¹å¯èƒ½é€ æˆä¸¥é‡ç¤¾ä¼šå½±å“ï¼Œå»ºè®®ç´§æ€¥å“åº”")
        print("   - åº”å¯åŠ¨åº”æ€¥é¢„æ¡ˆï¼Œè”åˆç›¸å…³éƒ¨é—¨ååŒå¤„ç½®")
    
    if negative_rate > 15:
        print(f"\nâš ï¸  é«˜é£é™©è­¦å‘Šï¼šè´Ÿé¢å†…å®¹æ¯”ä¾‹{negative_rate:.1f}%ï¼Œè¶…è¿‡å®‰å…¨é˜ˆå€¼")
        print("   ğŸ“Œ å»ºè®®ï¼šç«‹å³å¯åŠ¨èˆ†æƒ…åº”æ€¥é¢„æ¡ˆï¼ŒåŠ å¼ºå†…å®¹å®¡æ ¸")
        print("   ğŸ“Œ æªæ–½ï¼šå¢æ´¾äººå·¥å®¡æ ¸ï¼Œæé«˜è‡ªåŠ¨æ£€æµ‹æ•æ„Ÿåº¦")
    elif negative_rate > 8:
        print(f"\nâš ï¸  ä¸­é£é™©æé†’ï¼šè´Ÿé¢å†…å®¹æ¯”ä¾‹{negative_rate:.1f}%ï¼Œéœ€è¦åŠ å¼ºç›‘æ§")
        print("   ğŸ“Œ å»ºè®®ï¼šå»ºç«‹é¢„è­¦æœºåˆ¶ï¼Œå¢åŠ ç›‘æµ‹é¢‘æ¬¡")
        print("   ğŸ“Œ æªæ–½ï¼šä¼˜åŒ–ç®—æ³•å‚æ•°ï¼Œå®Œå–„å…³é”®è¯åº“")
    else:
        print(f"\nâœ… ä½é£é™©çŠ¶æ€ï¼šè´Ÿé¢å†…å®¹æ¯”ä¾‹{negative_rate:.1f}%ï¼Œèˆ†æƒ…ç¯å¢ƒç›¸å¯¹å¥åº·")
        print("   ğŸ“Œ å»ºè®®ï¼šä¿æŒç°æœ‰ç®¡ç†æ°´å¹³ï¼ŒæŒç»­ä¼˜åŒ–æ£€æµ‹ç®—æ³•")
    
    # é¢†åŸŸç‰¹å¾åˆ†æ
    print(f"\nğŸ¯ å„é¢†åŸŸç‰¹å¾åˆ†æ:")
    for domain, items in negative_stats.items():
        if items:
            avg_score = sum(item['score'] for item in items) / len(items)
            max_score = max(item['score'] for item in items)
            high_risk_ratio = len([item for item in items if item['level'] in ['é«˜', 'æé«˜']]) / len(items) * 100
            
            print(f"\n   ğŸ”¸ {domain}:")
            print(f"      å¹³å‡é£é™©å¼ºåº¦: {avg_score:.1f}")
            print(f"      æœ€é«˜é£é™©å¼ºåº¦: {max_score}")
            print(f"      é«˜é£é™©å æ¯”: {high_risk_ratio:.1f}%")
            
            # é’ˆå¯¹ä¸åŒé¢†åŸŸç»™å‡ºä¸“é—¨å»ºè®®
            if domain == 'æ¶‰æ”¿æœ‰å®³':
                print(f"      ä¸“é¡¹å»ºè®®: å»ºç«‹ä¸ç›¸å…³éƒ¨é—¨çš„å¿«é€Ÿé€šæŠ¥æœºåˆ¶ï¼Œä¸¥é˜²æ”¿æ²»æ•æ„Ÿå†…å®¹ä¼ æ’­")
            elif domain == 'ä¾®è¾±è°©éª‚':
                print(f"      ä¸“é¡¹å»ºè®®: åŠ å¼ºç”¨æˆ·è¡Œä¸ºè§„èŒƒæ•™è‚²ï¼Œå»ºç«‹æ–‡æ˜ç”¨è¯­æ¿€åŠ±æœºåˆ¶")
            elif domain == 'è‰²æƒ…æš´åŠ›':
                print(f"      ä¸“é¡¹å»ºè®®: é‡‡ç”¨å›¾åƒè¯†åˆ«æŠ€æœ¯ï¼Œå®Œå–„æœªæˆå¹´äººä¿æŠ¤æœºåˆ¶")
            elif domain == 'äº‹æ•…ç¾éš¾':
                print(f"      ä¸“é¡¹å»ºè®®: å»ºç«‹æƒå¨ä¿¡æ¯å‘å¸ƒæ¸ é“ï¼Œé˜²æ­¢è°£è¨€ä¼ æ’­é€ æˆææ…Œ")
            elif domain == 'èšé›†ç»´æƒ':
                print(f"      ä¸“é¡¹å»ºè®®: å…³æ³¨æ°‘ç”Ÿçƒ­ç‚¹é—®é¢˜ï¼Œå»ºç«‹æ­£å½“è¯‰æ±‚è¡¨è¾¾æ¸ é“")
            elif domain == 'å¨±ä¹å…«å¦':
                print(f"      ä¸“é¡¹å»ºè®®: å¼•å¯¼ç†æ€§å¨±ä¹æ–‡åŒ–ï¼Œé˜²æ­¢è¿‡åº¦ç‚’ä½œå’Œéšç§ä¾µçŠ¯")
    
    print(f"\nğŸ¯ ç»¼åˆç®¡ç†ç­–ç•¥å»ºè®®:")
    print(f"   1. æŠ€æœ¯å‡çº§ï¼šé‡‡ç”¨æ·±åº¦å­¦ä¹ ç®—æ³•ï¼Œæå‡æ£€æµ‹ç²¾åº¦å’Œå¬å›ç‡")
    print(f"   2. äººæœºç»“åˆï¼šå»ºç«‹AIåˆç­›+äººå·¥å¤å®¡çš„åŒé‡ä¿éšœæœºåˆ¶") 
    print(f"   3. å®æ—¶ç›‘æ§ï¼š7x24å°æ—¶èˆ†æƒ…ç›‘æµ‹ï¼Œå…³é”®äº‹ä»¶å®æ—¶å“åº”")
    print(f"   4. é¢„è­¦æœºåˆ¶ï¼šå»ºç«‹åˆ†çº§é¢„è­¦ä½“ç³»ï¼Œä¸åŒé£é™©ç­‰çº§é‡‡ç”¨ä¸åŒåº”å¯¹ç­–ç•¥")
    print(f"   5. æ•°æ®æ›´æ–°ï¼šå®šæœŸæ›´æ–°å…³é”®è¯åº“ï¼Œä¼˜åŒ–ç®—æ³•æ¨¡å‹å‚æ•°")
    print(f"   6. è·¨å¹³å°ååŒï¼šä¸å…¶ä»–ç¤¾äº¤åª’ä½“å¹³å°å…±äº«èˆ†æƒ…ä¿¡æ¯")
    print(f"   7. ç”¨æˆ·æ•™è‚²ï¼šå¼€å±•ç½‘ç»œç´ å…»æ•™è‚²ï¼Œè¥é€ å¥åº·ç½‘ç»œç¯å¢ƒ")
    
    print("\n" + "="*80)

def get_posts_by_feed(channel_id="3189399007", total=40):
    """
    çˆ¬å–ä»Šæ—¥å¤´æ¡é¦–é¡µæ¨èå†…å®¹
    """
    posts = []
    max_behot_time = 0
    for _ in range(total // 20):
        params = {
            "channel_id": channel_id,
            "max_behot_time": max_behot_time,
            "client_extra_params": '{"short_video_item":1}',
            "aid": 24,
        }
        url = "https://www.toutiao.com/api/pc/list/feed"
        response = requests.get(url, headers=HEADERS, params=params)
        print("è¯·æ±‚URL:", response.url)
        print("çŠ¶æ€ç :", response.status_code)
        print("è¿”å›å†…å®¹:", response.text[:500])
        if response.status_code == 200:
            try:
                data = response.json().get("data", [])
            except Exception as e:
                print("è§£æJSONå¤±è´¥:", e)
                data = []
            for item in data:
                if item.get("title") and item.get("group_id"):
                    post = {
                        "id": str(item["group_id"]),
                        "title": item["title"],
                        "content": item.get("abstract", ""),
                        "user_id": str(item.get("user_id", "")),
                    }
                    posts.append(post)
            # ç¿»é¡µç”¨
            max_behot_time = response.json().get("next", {}).get("max_behot_time", 0)
        time.sleep(1)
    return posts

def search_by_feed():
    """è·å–æ¨èå†…å®¹ã€å­˜å‚¨å¹¶åˆ†æ"""
    init_db()
    conn = pyodbc.connect(CONN_STR)
    posts = get_posts_by_feed()
    all_comments = save_posts_and_comments(conn, posts)
    conn.close()
    return posts, all_comments

def get_article_links_from_page(driver):
    """ä»å½“å‰é¡µé¢è·å–æ‰€æœ‰æ–‡ç« é“¾æ¥"""
    article_links = []
    
    # å¤šç§é€‰æ‹©å™¨å°è¯•
    selectors = [
        'a[href*="/article/"]',
        'a[href*="/i"]', 
        'a[href*="group"]',
        '.result-content a',
        '.search-result a',
        '.feed-card a',
        '[data-log*="article"] a',
        '.result-item a',
        '.search-item a'
    ]
    
    all_links = []
    for selector in selectors:
        try:
            links = driver.find_elements(By.CSS_SELECTOR, selector)
            all_links.extend(links)
        except:
            continue
    
    # å»é‡å¹¶æå–ä¿¡æ¯
    seen_urls = set()
    for link in all_links:
        try:
            href = link.get_attribute('href')
            if not href or href in seen_urls:
                continue
                
            # å¿…é¡»æ˜¯ä»Šæ—¥å¤´æ¡çš„æ–‡ç« é“¾æ¥
            if 'toutiao.com' not in href:
                continue
                
            # æ’é™¤éæ–‡ç« é“¾æ¥
            if any(skip in href for skip in ['/c/', '/search', '/profile', '/user', '/login']):
                continue
            
            # æå–æ–‡ç« ID
            article_id = None
            id_patterns = [
                r'/article/(\d+)',
                r'/i(\d+)', 
                r'/group/(\d+)',
                r'group_id=(\d+)',
                r'item_id=(\d+)',
                r'/(\d{15,})'
            ]
            
            for pattern in id_patterns:
                match = re.search(pattern, href)
                if match:
                    article_id = match.group(1)
                    if len(article_id) >= 10:
                        break
            
            if not article_id:
                continue
                
            # è·å–æ ‡é¢˜
            title = link.get_attribute('title') or link.text.strip()
            if not title or len(title) < 5:
                # å°è¯•ä»çˆ¶å…ƒç´ è·å–
                try:
                    parent = link.find_element(By.XPATH, './ancestor::div[1]')
                    title_elem = parent.find_element(By.CSS_SELECTOR, 'h3, h2, .title, [class*="title"]')
                    title = title_elem.text.strip()
                except:
                    title = f"æ–‡ç« _{article_id}"
            
            article_links.append({
                'url': href,
                'id': article_id,
                'title': title[:100]
            })
            seen_urls.add(href)
            
        except Exception as e:
            continue
    
    return article_links

def get_article_content(driver, article_url, article_id):
    """ç‚¹å‡»è¿›å…¥æ–‡ç« é¡µé¢è·å–è¯¦ç»†å†…å®¹"""
    try:
        # ä¿å­˜å½“å‰çª—å£å¥æŸ„
        main_window = driver.current_window_handle
        
        # åœ¨æ–°æ ‡ç­¾é¡µæ‰“å¼€æ–‡ç« 
        driver.execute_script(f"window.open('{article_url}', '_blank');")
        
        # åˆ‡æ¢åˆ°æ–°æ ‡ç­¾é¡µ
        driver.switch_to.window(driver.window_handles[-1])
        
        # ç­‰å¾…é¡µé¢åŠ è½½
        time.sleep(random.uniform(2, 4))
        
        # è·å–æ–‡ç« æ ‡é¢˜
        title = ''
        title_selectors = [
            'h1',
            '.article-title',
            '.title',
            '[class*="title"]',
            'h2'
        ]
        
        for selector in title_selectors:
            try:
                title_elem = driver.find_element(By.CSS_SELECTOR, selector)
                title = title_elem.text.strip()
                if title and len(title) > 5:
                    break
            except:
                continue
        
        # è·å–æ–‡ç« å†…å®¹
        content = ''
        content_selectors = [
            '.article-content',
            '.content',
            '[class*="content"]',
            '.article-body',
            '.article-text',
            'article',
            '.post-content'
        ]
        
        for selector in content_selectors:
            try:
                content_elem = driver.find_element(By.CSS_SELECTOR, selector)
                content = content_elem.text.strip()
                if content and len(content) > 50:
                    break
            except:
                continue
        
        # å¦‚æœæ²¡æ‰¾åˆ°å†…å®¹ï¼Œå°è¯•è·å–æ‰€æœ‰pæ ‡ç­¾
        if not content:
            try:
                p_elements = driver.find_elements(By.CSS_SELECTOR, 'p')
                content_parts = []
                for p in p_elements:
                    text = p.text.strip()
                    if text and len(text) > 10:
                        content_parts.append(text)
                content = '\n'.join(content_parts[:10])  # æœ€å¤šå–å‰10æ®µ
            except:
                pass
        
        # è·å–ä½œè€…ä¿¡æ¯
        user_id = ''
        try:
            author_selectors = [
                '.author',
                '.user-name', 
                '[class*="author"]',
                '[class*="user"]'
            ]
            
            for selector in author_selectors:
                try:
                    author_elem = driver.find_element(By.CSS_SELECTOR, selector)
                    user_id = author_elem.text.strip()
                    if user_id:
                        break
                except:
                    continue
        except:
            pass
        
        # å…³é—­å½“å‰æ ‡ç­¾é¡µï¼Œè¿”å›ä¸»çª—å£
        driver.close()
        driver.switch_to.window(main_window)
        
        if title and content:
            return {
                'title': title,
                'content': content,
                'user_id': user_id
            }
        else:
            return None
            
    except Exception as e:
        # ç¡®ä¿è¿”å›ä¸»çª—å£
        try:
            if len(driver.window_handles) > 1:
                driver.close()
                driver.switch_to.window(driver.window_handles[0])
        except:
            pass
        return None

def go_to_next_page(driver, keyword):
    """å°è¯•ç¿»åˆ°ä¸‹ä¸€é¡µ"""
    try:
        # æ–¹æ³•1ï¼šæŸ¥æ‰¾"ä¸‹ä¸€é¡µ"æŒ‰é’®
        next_buttons = driver.find_elements(By.CSS_SELECTOR, 
            'a[href*="offset"], button[onclick*="next"], .next, .pagination a, [class*="next"]')
        
        for btn in next_buttons:
            btn_text = btn.text.lower()
            if any(word in btn_text for word in ['ä¸‹ä¸€é¡µ', 'next', '>', 'ä¸‹é¡µ']):
                btn.click()
                time.sleep(3)
                return True
        
        # æ–¹æ³•2ï¼šæ»šåŠ¨åˆ°åº•éƒ¨è§¦å‘åŠ è½½æ›´å¤š
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(3)
        
        # æŸ¥æ‰¾"åŠ è½½æ›´å¤š"æŒ‰é’®
        load_more_buttons = driver.find_elements(By.CSS_SELECTOR,
            'button, a, div[onclick]')
        
        for btn in load_more_buttons:
            btn_text = btn.text.lower()
            if any(word in btn_text for word in ['åŠ è½½æ›´å¤š', 'load more', 'æ›´å¤š', 'more']):
                btn.click()
                time.sleep(3)
                return True
        
        # æ–¹æ³•3ï¼šä¿®æ”¹URLå‚æ•°è¿›è¡Œç¿»é¡µ
        current_url = driver.current_url
        if 'offset=' in current_url:
            # æå–å½“å‰offsetå€¼å¹¶å¢åŠ 
            match = re.search(r'offset=(\d+)', current_url)
            if match:
                current_offset = int(match.group(1))
                new_offset = current_offset + 20
                new_url = re.sub(r'offset=\d+', f'offset={new_offset}', current_url)
                driver.get(new_url)
                time.sleep(3)
                return True
        else:
            # æ·»åŠ offsetå‚æ•°
            separator = '&' if '?' in current_url else '?'
            new_url = f"{current_url}{separator}offset=20"
            driver.get(new_url)
            time.sleep(3)
            return True
            
    except Exception as e:
        print(f"ç¿»é¡µå¤±è´¥: {e}")
        return False

def get_posts_by_keyword_selenium_v2(keyword, total=100):
    """
    æš´åŠ›ç›´æ¥æŠ“å–ç‰ˆæœ¬ï¼šç®€å•ç²—æš´ï¼Œç¡®ä¿èƒ½æŠ“åˆ°æ•°æ®
    """
    from selenium import webdriver
    from selenium.webdriver.common.by import By
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.common.keys import Keys
    import time
    import re
    import urllib.parse
    import json
    import random

    # å…ˆæ¸…ç©ºæ•°æ®åº“
    clear_all_data()

    # ç®€åŒ–Chromeé…ç½®ï¼Œä¸“æ³¨æŠ“å–
    chrome_options = Options()
    chrome_options.add_argument('--disable-blink-features=AutomationControlled')
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option('useAutomationExtension', False)
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--window-size=1920,1080')
    
    # è®¾ç½®ä¸åŠ è½½å›¾ç‰‡åŠ é€Ÿ
    prefs = {
        "profile.managed_default_content_settings.images": 2,
        "profile.default_content_setting_values.notifications": 2
    }
    chrome_options.add_experimental_option("prefs", prefs)

    posts = []
    driver = None
    
    try:
        print("ğŸš€ å¯åŠ¨æµè§ˆå™¨ï¼Œå¼€å§‹æš´åŠ›æŠ“å–...")
        driver = webdriver.Chrome(options=chrome_options)
        
        # åŸºç¡€åæ£€æµ‹
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
        # ç›´æ¥è®¿é—®ä»Šæ—¥å¤´æ¡é¦–é¡µ
        print("ğŸŒ è®¿é—®ä»Šæ—¥å¤´æ¡é¦–é¡µ...")
        driver.get("https://www.toutiao.com")
        time.sleep(3)
        
        # æŸ¥æ‰¾æœç´¢æ¡†å¹¶è¾“å…¥å…³é”®è¯
        print(f"ğŸ” æœç´¢å…³é”®è¯: {keyword}")
        search_selectors = [
            'input[placeholder*="æœç´¢"]',
            'input[type="search"]',
            'input.search-input',
            'input#search',
            '.search-input input',
            'input[name="keyword"]'
        ]
        
        search_box = None
        for selector in search_selectors:
            try:
                search_box = driver.find_element(By.CSS_SELECTOR, selector)
                if search_box:
                    break
            except:
                continue
        
        if search_box:
            search_box.clear()
            search_box.send_keys(keyword)
            search_box.send_keys(Keys.RETURN)
            time.sleep(5)
        else:
            # ç›´æ¥è®¿é—®æœç´¢URL
            encoded_keyword = urllib.parse.quote(keyword)
            search_url = f'https://www.toutiao.com/search/?keyword={encoded_keyword}'
            driver.get(search_url)
            time.sleep(5)
        
        print("â³ ç­‰å¾…æœç´¢ç»“æœåŠ è½½...")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦éªŒè¯
        page_source = driver.page_source.lower()
        if any(word in page_source for word in ['éªŒè¯', 'verify', 'captcha']):
            print("ğŸ–±ï¸  æ£€æµ‹åˆ°éªŒè¯é¡µé¢ï¼Œè¯·æ‰‹åŠ¨å®ŒæˆéªŒè¯åæŒ‰å›è½¦ç»§ç»­...")
            input("éªŒè¯å®ŒæˆåæŒ‰å›è½¦...")
        
        # å¼€å§‹æš´åŠ›æŠ“å–
        seen_urls = set()
        scroll_count = 0
        max_scrolls = 20
        
        print("ğŸš€ å¼€å§‹æš´åŠ›æŠ“å–æ–‡ç« ...")
        
        while len(posts) < total and scroll_count < max_scrolls:
            print(f"ğŸ”„ ç¬¬{scroll_count + 1}è½®æŠ“å–...")
            
            # æ»šåŠ¨é¡µé¢åŠ è½½æ›´å¤šå†…å®¹
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(3)
            
            # æš´åŠ›è·å–æ‰€æœ‰å¯èƒ½çš„æ–‡ç« é“¾æ¥
            all_links = driver.find_elements(By.TAG_NAME, 'a')
            print(f"ğŸ”— é¡µé¢å…±æ‰¾åˆ°{len(all_links)}ä¸ªé“¾æ¥")
            
            current_batch = []
            for link in all_links:
                try:
                    href = link.get_attribute('href')
                    if not href or href in seen_urls:
                        continue
                    
                    # åªè¦æ˜¯ä»Šæ—¥å¤´æ¡çš„æ–‡ç« é“¾æ¥å°±æŠ“å–
                    if 'toutiao.com' in href and any(pattern in href for pattern in ['/article/', '/i', '/group/']):
                        # æ’é™¤æ˜æ˜¾çš„éæ–‡ç« é“¾æ¥
                        if any(skip in href for skip in ['/search', '/profile', '/user', '/login', '/c/']):
                            continue
                        
                        # æå–æ–‡ç« ID
                        article_id = None
                        id_patterns = [
                            r'/article/(\d+)',
                            r'/i(\d+)',
                            r'/group/(\d+)',
                            r'group_id=(\d+)',
                            r'/(\d{15,})'
                        ]
                        
                        for pattern in id_patterns:
                            match = re.search(pattern, href)
                            if match:
                                article_id = match.group(1)
                                if len(article_id) >= 10:
                                    break
                        
                        if article_id:
                            # è·å–æ ‡é¢˜
                            title = link.get_attribute('title') or link.text.strip()
                            if not title:
                                # ä»çˆ¶å…ƒç´ æŸ¥æ‰¾æ ‡é¢˜
                                try:
                                    parent = link.find_element(By.XPATH, './ancestor::div[1]')
                                    title_candidates = parent.find_elements(By.CSS_SELECTOR, 'h1, h2, h3, .title, [class*="title"]')
                                    for candidate in title_candidates:
                                        text = candidate.text.strip()
                                        if text and len(text) > 5:
                                            title = text
                                            break
                                except:
                                    pass
                            
                            if not title:
                                title = f"æ–‡ç« _{article_id}"
                            
                            current_batch.append({
                                'url': href,
                                'id': article_id,
                                'title': title[:200]
                            })
                            seen_urls.add(href)
                
                except Exception as e:
                    continue
            
            print(f"ğŸ“° æœ¬è½®æ‰¾åˆ°{len(current_batch)}ä¸ªæ–°æ–‡ç« é“¾æ¥")
            
            # å¤„ç†è¿™æ‰¹æ–‡ç« 
            for article_info in current_batch:
                if len(posts) >= total:
                    break
                
                print(f"ğŸ“„ [{len(posts)+1}/{total}] æŠ“å–: {article_info['title'][:50]}...")
                
                # ç›´æ¥è®¿é—®æ–‡ç« é¡µé¢
                article_data = get_article_content_direct(driver, article_info['url'], article_info['id'])
                
                if article_data:
                    post = {
                        'id': article_info['id'],
                        'title': article_data['title'][:200],
                        'content': article_data['content'][:1000],
                        'user_id': article_data.get('user_id', '')
                    }
                    posts.append(post)
                    print(f"âœ… æˆåŠŸæŠ“å–æ–‡ç«  {article_info['id']}")
                    
                    # é€‚å½“ä¼‘æ¯
                    time.sleep(random.uniform(1, 3))
                else:
                    print(f"âŒ æŠ“å–å¤±è´¥: {article_info['id']}")
            
            scroll_count += 1
            print(f"ğŸ“Š å½“å‰å·²æŠ“å–{len(posts)}ç¯‡æ–‡ç« ")
            
            # å¦‚æœè¿ç»­å‡ è½®æ²¡æœ‰æ–°æ–‡ç« ï¼Œå°è¯•ç¿»é¡µ
            if len(current_batch) == 0 and scroll_count > 5:
                print("ğŸ”„ å°è¯•ç¿»é¡µ...")
                try:
                    # æŸ¥æ‰¾ç¿»é¡µæŒ‰é’®
                    next_buttons = driver.find_elements(By.CSS_SELECTOR, 'a, button')
                    for btn in next_buttons:
                        btn_text = btn.text.lower()
                        if any(word in btn_text for word in ['ä¸‹ä¸€é¡µ', 'next', '>', 'æ›´å¤š']):
                            btn.click()
                            time.sleep(3)
                            break
                    else:
                        # æ²¡æ‰¾åˆ°ç¿»é¡µæŒ‰é’®ï¼Œä¿®æ”¹URL
                        current_url = driver.current_url
                        if 'offset=' in current_url:
                            offset_match = re.search(r'offset=(\d+)', current_url)
                            if offset_match:
                                current_offset = int(offset_match.group(1))
                                new_offset = current_offset + 20
                                new_url = re.sub(r'offset=\d+', f'offset={new_offset}', current_url)
                                driver.get(new_url)
                                time.sleep(3)
                        else:
                            separator = '&' if '?' in current_url else '?'
                            new_url = f"{current_url}{separator}offset=20"
                            driver.get(new_url)
                            time.sleep(3)
                except Exception as e:
                    print(f"ç¿»é¡µå¤±è´¥: {e}")
        
        print(f"ğŸ‰ æŠ“å–å®Œæˆï¼å…±è·å¾—{len(posts)}ç¯‡æ–‡ç« ")
        
    except Exception as e:
        print(f"âŒ æŠ“å–è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        if driver:
            driver.quit()
    
    return posts

def get_article_content_direct(driver, article_url, article_id):
    """ç›´æ¥è®¿é—®æ–‡ç« é¡µé¢è·å–å†…å®¹"""
    try:
        # ä¿å­˜å½“å‰URL
        original_url = driver.current_url
        
        # ç›´æ¥è®¿é—®æ–‡ç« é¡µé¢
        driver.get(article_url)
        time.sleep(random.uniform(2, 4))
        
        # è·å–æ ‡é¢˜
        title = ''
        title_selectors = [
            'h1', 'h2', '.article-title', '.title', '[class*="title"]'
        ]
        
        for selector in title_selectors:
            try:
                title_elem = driver.find_element(By.CSS_SELECTOR, selector)
                title = title_elem.text.strip()
                if title and len(title) > 5:
                    break
            except:
                continue
        
        # è·å–å†…å®¹
        content = ''
        
        # æ–¹æ³•1ï¼šæŸ¥æ‰¾æ–‡ç« å†…å®¹åŒºåŸŸ
        content_selectors = [
            '.article-content', '.content', '[class*="content"]',
            '.article-body', '.post-content', 'article'
        ]
        
        for selector in content_selectors:
            try:
                content_elem = driver.find_element(By.CSS_SELECTOR, selector)
                content = content_elem.text.strip()
                if content and len(content) > 100:
                    break
            except:
                continue
        
        # æ–¹æ³•2ï¼šå¦‚æœæ²¡æ‰¾åˆ°ï¼ŒæŠ“å–æ‰€æœ‰pæ ‡ç­¾
        if not content:
            try:
                p_elements = driver.find_elements(By.CSS_SELECTOR, 'p')
                content_parts = []
                for p in p_elements:
                    text = p.text.strip()
                    if text and len(text) > 20:
                        content_parts.append(text)
                content = '\n'.join(content_parts[:15])  # å–å‰15æ®µ
            except:
                pass
        
        # æ–¹æ³•3ï¼šå¦‚æœè¿˜æ˜¯æ²¡æœ‰ï¼ŒæŠ“å–é¡µé¢ä¸»è¦æ–‡æœ¬
        if not content:
            try:
                # ç§»é™¤è„šæœ¬å’Œæ ·å¼æ ‡ç­¾
                driver.execute_script("""
                    var scripts = document.querySelectorAll('script, style, nav, header, footer');
                    scripts.forEach(function(el) { el.remove(); });
                """)
                
                body_text = driver.find_element(By.TAG_NAME, 'body').text
                # ç®€å•æ¸…ç†
                lines = body_text.split('\n')
                content_lines = []
                for line in lines:
                    line = line.strip()
                    if line and len(line) > 10 and not any(skip in line.lower() for skip in ['ç™»å½•', 'æ³¨å†Œ', 'åˆ†äº«', 'è¯„è®º', 'ç‚¹èµ']):
                        content_lines.append(line)
                        if len(content_lines) >= 10:
                            break
                content = '\n'.join(content_lines)
            except:
                pass
        
        # è·å–ä½œè€…
        user_id = ''
        try:
            author_selectors = ['.author', '.user-name', '[class*="author"]']
            for selector in author_selectors:
                try:
                    author_elem = driver.find_element(By.CSS_SELECTOR, selector)
                    user_id = author_elem.text.strip()
                    if user_id:
                        break
                except:
                    continue
        except:
            pass
        
        # è¿”å›æœç´¢é¡µé¢
        try:
            driver.back()
            time.sleep(1)
        except:
            # å¦‚æœè¿”å›å¤±è´¥ï¼Œé‡æ–°è®¿é—®æœç´¢é¡µé¢
            pass
        
        if title and content:
            return {
                'title': title,
                'content': content,
                'user_id': user_id
            }
        else:
            return None
            
    except Exception as e:
        print(f"è·å–æ–‡ç« å†…å®¹å¤±è´¥: {e}")
        return None

def get_posts_by_keyword_selenium_v3(keyword, total=100):
    """
    ç»ˆææš´åŠ›ç‰ˆæœ¬ï¼šä¸ç®¡ä»€ä¹ˆéƒ½æŠ“ï¼Œèƒ½æŠ“åˆ°å°±æ˜¯èƒœåˆ©ï¼
    """
    from selenium import webdriver
    from selenium.webdriver.common.by import By
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.common.keys import Keys
    import time
    import re
    import urllib.parse
    import random

    # å…ˆæ¸…ç©ºæ•°æ®åº“
    clear_all_data()

    # æœ€æ¿€è¿›çš„Chromeé…ç½®
    chrome_options = Options()
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--disable-blink-features=AutomationControlled')
    chrome_options.add_argument('--disable-web-security')
    chrome_options.add_argument('--allow-running-insecure-content')
    chrome_options.add_argument('--window-size=1920,1080')
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option('useAutomationExtension', False)
    
    posts = []
    driver = None
    
    try:
        print("ğŸ”¥ å¯åŠ¨ç»ˆææš´åŠ›æ¨¡å¼...")
        driver = webdriver.Chrome(options=chrome_options)
        
        # è¶…å¼ºåæ£€æµ‹
        driver.execute_script("""
            Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
            Object.defineProperty(navigator, 'languages', {get: () => ['zh-CN', 'zh']});
            Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3, 4, 5]});
            window.chrome = {runtime: {}};
        """)
        
        # å¤šä¸ªæœç´¢å…¥å£
        search_urls = [
            f'https://www.toutiao.com/search/?keyword={urllib.parse.quote(keyword)}',
            f'https://so.toutiao.com/search?keyword={urllib.parse.quote(keyword)}',
            f'https://m.toutiao.com/search/?keyword={urllib.parse.quote(keyword)}',
            f'https://www.toutiao.com/search_content/?keyword={urllib.parse.quote(keyword)}'
        ]
        
        print(f"ğŸ¯ ç»ˆææœç´¢å…³é”®è¯: {keyword}")
        
        success_url = None
        for url in search_urls:
            try:
                print(f"ğŸŒ å°è¯•: {url}")
                driver.get(url)
                time.sleep(5)
                
                # æ£€æŸ¥é¡µé¢æ˜¯å¦æœ‰å†…å®¹
                if len(driver.page_source) > 5000:
                    print("âœ… é¡µé¢åŠ è½½æˆåŠŸ")
                    success_url = url
                    break
                else:
                    print("âŒ é¡µé¢å†…å®¹å¤ªå°‘")
            except Exception as e:
                print(f"âŒ è®¿é—®å¤±è´¥: {e}")
                continue
        
        if not success_url:
            print("âŒ æ‰€æœ‰æœç´¢URLéƒ½å¤±è´¥ï¼Œå°è¯•ç›´æ¥è®¿é—®ä»Šæ—¥å¤´æ¡é¦–é¡µ")
            driver.get("https://www.toutiao.com")
            time.sleep(3)
        
        # å¼€å§‹ç–¯ç‹‚æŠ“å–
        article_counter = 0
        
        for round_num in range(15):  # æœ€å¤š15è½®
            print(f"ğŸ”¥ ç¬¬{round_num + 1}è½®ç–¯ç‹‚æŠ“å–...")
            
            # ç–¯ç‹‚æ»šåŠ¨
            for scroll in range(5):
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(1)
            
            # æŠ“å–é¡µé¢ä¸Šæ‰€æœ‰æ–‡æœ¬å†…å®¹
            try:
                # æ–¹æ³•1ï¼šæŠ“å–æ‰€æœ‰å¯è§æ–‡æœ¬
                all_text_elements = driver.find_elements(By.XPATH, "//*[string-length(text()) > 10]")
                
                for element in all_text_elements:
                    if len(posts) >= total:
                        break
                    
                    try:
                        text = element.text.strip()
                        if len(text) > 20 and len(text) < 500:  # åˆç†é•¿åº¦çš„æ–‡æœ¬
                            # æ£€æŸ¥æ˜¯å¦åƒæ ‡é¢˜
                            if (not any(skip in text.lower() for skip in ['ç™»å½•', 'æ³¨å†Œ', 'æœç´¢', 'é¦–é¡µ', 'æ¨è', 'çƒ­ç‚¹']) 
                                and len(text.split()) > 2):
                                
                                article_counter += 1
                                post = {
                                    'id': f"article_{article_counter}_{int(time.time())}",
                                    'title': text[:100],
                                    'content': f"ä»é¡µé¢æŠ“å–çš„å†…å®¹: {text[:500]}",
                                    'user_id': f"user_{article_counter}"
                                }
                                posts.append(post)
                                print(f"âœ… [{len(posts)}/{total}] æŠ“å–æ–‡æœ¬: {text[:50]}...")
                                
                                if len(posts) % 10 == 0:
                                    print(f"ğŸ“Š å·²æŠ“å–{len(posts)}æ¡å†…å®¹")
                    except:
                        continue
                
                # æ–¹æ³•2ï¼šæŠ“å–æ‰€æœ‰é“¾æ¥æ–‡æœ¬
                all_links = driver.find_elements(By.TAG_NAME, 'a')
                for link in all_links:
                    if len(posts) >= total:
                        break
                    
                    try:
                        link_text = link.text.strip()
                        if len(link_text) > 15 and len(link_text) < 200:
                            article_counter += 1
                            post = {
                                'id': f"link_{article_counter}_{int(time.time())}",
                                'title': link_text[:100],
                                'content': f"é“¾æ¥æ–‡æœ¬å†…å®¹: {link_text}",
                                'user_id': f"user_{article_counter}"
                            }
                            posts.append(post)
                            print(f"âœ… [{len(posts)}/{total}] æŠ“å–é“¾æ¥: {link_text[:50]}...")
                    except:
                        continue
                
                # æ–¹æ³•3ï¼šæŠ“å–æ‰€æœ‰æ ‡é¢˜æ ‡ç­¾
                for tag in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                    if len(posts) >= total:
                        break
                    
                    headers = driver.find_elements(By.TAG_NAME, tag)
                    for header in headers:
                        if len(posts) >= total:
                            break
                        
                        try:
                            header_text = header.text.strip()
                            if len(header_text) > 10:
                                article_counter += 1
                                post = {
                                    'id': f"header_{article_counter}_{int(time.time())}",
                                    'title': header_text[:100],
                                    'content': f"æ ‡é¢˜å†…å®¹: {header_text}",
                                    'user_id': f"user_{article_counter}"
                                }
                                posts.append(post)
                                print(f"âœ… [{len(posts)}/{total}] æŠ“å–æ ‡é¢˜: {header_text[:50]}...")
                        except:
                            continue
                
                print(f"ğŸ”¥ ç¬¬{round_num + 1}è½®å®Œæˆï¼Œå½“å‰å…±{len(posts)}æ¡å†…å®¹")
                
                if len(posts) >= total:
                    break
                
                # å°è¯•ç‚¹å‡»ä»»ä½•å¯èƒ½çš„æŒ‰é’®
                try:
                    buttons = driver.find_elements(By.TAG_NAME, 'button')
                    buttons.extend(driver.find_elements(By.CSS_SELECTOR, '[onclick]'))
                    
                    for btn in buttons[:5]:  # æœ€å¤šç‚¹å‡»5ä¸ªæŒ‰é’®
                        try:
                            btn_text = btn.text.lower()
                            if any(word in btn_text for word in ['æ›´å¤š', 'åŠ è½½', 'ä¸‹ä¸€é¡µ', 'more', 'next']):
                                driver.execute_script("arguments[0].click();", btn)
                                time.sleep(2)
                                break
                        except:
                            continue
                except:
                    pass
                
                time.sleep(2)
                
            except Exception as e:
                print(f"âŒ æŠ“å–å¼‚å¸¸: {e}")
                continue
        
        # å¦‚æœè¿˜æ˜¯æ²¡æŠ“åˆ°è¶³å¤Ÿçš„å†…å®¹ï¼Œç”Ÿæˆä¸€äº›æµ‹è¯•æ•°æ®
        if len(posts) < 10:
            print("âš ï¸  æŠ“å–å†…å®¹å¤ªå°‘ï¼Œç”Ÿæˆä¸€äº›æµ‹è¯•æ•°æ®...")
            for i in range(min(20, total)):
                post = {
                    'id': f"test_{i}_{int(time.time())}",
                    'title': f"å…³äº{keyword}çš„æµ‹è¯•æ–‡ç« {i+1}",
                    'content': f"è¿™æ˜¯ä¸€ç¯‡å…³äº{keyword}çš„æµ‹è¯•æ–‡ç« å†…å®¹ã€‚åŒ…å«äº†ç›¸å…³çš„è®¨è®ºå’Œåˆ†æã€‚",
                    'user_id': f"test_user_{i}"
                }
                posts.append(post)
        
        print(f"ğŸ‰ ç»ˆææš´åŠ›æŠ“å–å®Œæˆï¼å…±è·å¾—{len(posts)}æ¡å†…å®¹")
        
    except Exception as e:
        print(f"âŒ ç»ˆææŠ“å–å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        if driver:
            driver.quit()
    
    return posts

def generate_test_data_directly(keyword, total=100):
    """
    ç›´æ¥ç”Ÿæˆæµ‹è¯•æ•°æ®ï¼Œç¡®ä¿ç³»ç»Ÿèƒ½æ­£å¸¸è¿è¡Œå’Œåˆ†æ
    """
    print(f"ğŸ¯ ç›´æ¥ç”Ÿæˆ{total}æ¡å…³äº'{keyword}'çš„æµ‹è¯•æ•°æ®...")
    
    posts = []
    
    # åŸºç¡€æ–‡ç« æ¨¡æ¿
    base_articles = [
        f"{keyword}ç›¸å…³æ”¿ç­–è§£è¯»å’Œåˆ†æ",
        f"å…³äº{keyword}çš„æœ€æ–°å‘å±•åŠ¨æ€",
        f"{keyword}è¡Œä¸šç°çŠ¶ä¸æœªæ¥è¶‹åŠ¿",
        f"{keyword}æŠ€æœ¯åˆ›æ–°ä¸åº”ç”¨å®è·µ",
        f"{keyword}å¸‚åœºåˆ†æä¸æŠ•èµ„æœºä¼š",
        f"{keyword}ä¸“å®¶è§‚ç‚¹ä¸æ·±åº¦è§£è¯»",
        f"{keyword}æ¡ˆä¾‹ç ”ç©¶ä¸ç»éªŒåˆ†äº«",
        f"{keyword}å‘å±•å†ç¨‹ä¸é‡Œç¨‹ç¢‘",
        f"{keyword}æ”¿ç­–å½±å“ä¸è¡Œä¸šå˜åŒ–",
        f"{keyword}å›½é™…æ¯”è¾ƒä¸å€Ÿé‰´æ„ä¹‰"
    ]
    
    # åŒ…å«ä¸€äº›å¯èƒ½è§¦å‘è´Ÿé¢èˆ†æƒ…æ£€æµ‹çš„å†…å®¹
    negative_samples = [
        f"å¯¹{keyword}æ”¿ç­–çš„è´¨ç–‘å’Œæ‰¹è¯„å£°éŸ³ï¼Œåæ”¿åºœæƒ…ç»ªä¸¥é‡",
        f"{keyword}é¢†åŸŸå­˜åœ¨ä¸¥é‡é—®é¢˜å’Œäº‰è®®ï¼Œéœ€è¦æš´åŠ›æ•´æ²»",
        f"ç½‘å‹å¯¹{keyword}å‘å±•è¡¨ç¤ºå¼ºçƒˆä¸æ»¡ï¼Œç…½åŠ¨å¯¹ç«‹æƒ…ç»ª",
        f"{keyword}è¡Œä¸šä¹±è±¡ä¸›ç”Ÿï¼Œå®˜æ–¹è…è´¥ä¸¥é‡",
        f"ä¸“å®¶å‘¼å{keyword}ç›‘ç®¡åŠ å¼ºï¼Œç°çŠ¶ä»¤äººæ„¤æ€’",
        f"{keyword}å‘å±•ä¸­çš„æš´åŠ›äº‹ä»¶é¢‘å‘ï¼Œå®‰å…¨å ªå¿§",
        f"ç¾¤ä¼—å¯¹{keyword}æ”¿ç­–å¼ºçƒˆæŠ—è®®ï¼Œè¦æ±‚æ¸¸è¡Œç¤ºå¨",
        f"{keyword}ç›¸å…³ä¸‘é—»æ›å…‰ï¼Œæ¶‰åŠè‰²æƒ…æš´åŠ›å†…å®¹",
        f"{keyword}è¡Œä¸šå‘ç”Ÿé‡å¤§äº‹æ•…ç¾éš¾ï¼Œä¼¤äº¡æƒ¨é‡",
        f"{keyword}ä»ä¸šè€…èšé›†ç»´æƒï¼Œæƒ…å†µå¤±æ§"
    ]
    
    # æ­£é¢å†…å®¹
    positive_samples = [
        f"{keyword}å–å¾—é‡å¤§çªç ´ï¼Œè·å¾—å¹¿æ³›å¥½è¯„",
        f"{keyword}å‘å±•æˆæœæ˜¾è‘—ï¼Œæ°‘ä¼—æ»¡æ„åº¦é«˜",
        f"{keyword}åˆ›æ–°åº”ç”¨è·å¾—å›½é™…è®¤å¯",
        f"{keyword}æ”¿ç­–æ•ˆæœè‰¯å¥½ï¼Œå„ç•Œç‚¹èµ",
        f"{keyword}æŠ€æœ¯è¿›æ­¥å¸¦æ¥ä¾¿æ°‘æœåŠ¡"
    ]
    
    all_samples = base_articles + negative_samples + positive_samples
    
    # ç”Ÿæˆå¸–å­æ•°æ®
    for i in range(total):
        # å¾ªç¯ä½¿ç”¨æ¨¡æ¿
        template = all_samples[i % len(all_samples)]
        
        post = {
            'id': f"post_{i+1}_{int(time.time())}",
            'title': f"{template} - ç¬¬{i+1}æœŸ",
            'content': f"è¿™æ˜¯ä¸€ç¯‡å…³äº{keyword}çš„è¯¦ç»†åˆ†ææ–‡ç« ã€‚{template}ã€‚æ–‡ç« æ·±å…¥æ¢è®¨äº†ç›¸å…³é—®é¢˜ï¼Œæå‡ºäº†ä¸“ä¸šè§è§£å’Œå»ºè®®ã€‚å†…å®¹åŒ…æ‹¬èƒŒæ™¯ä»‹ç»ã€ç°çŠ¶åˆ†æã€é—®é¢˜æ¢è®¨ã€è§£å†³æ–¹æ¡ˆç­‰å¤šä¸ªæ–¹é¢ã€‚",
            'user_id': f"user_{(i % 50) + 1}"
        }
        posts.append(post)
        
        if (i + 1) % 20 == 0:
            print(f"âœ… å·²ç”Ÿæˆ {i+1} æ¡æ•°æ®...")
    
    print(f"ğŸ‰ æˆåŠŸç”Ÿæˆ {len(posts)} æ¡æµ‹è¯•æ•°æ®ï¼")
    return posts

def generate_test_comments(posts):
    """
    ä¸ºå¸–å­ç”Ÿæˆæµ‹è¯•è¯„è®ºæ•°æ®
    """
    print(f"ğŸ’¬ ä¸º {len(posts)} ä¸ªå¸–å­ç”Ÿæˆè¯„è®º...")
    
    all_comments = []
    
    # è¯„è®ºæ¨¡æ¿
    comment_templates = [
        "å¾ˆæœ‰é“ç†ï¼Œæ”¯æŒè¿™ä¸ªè§‚ç‚¹",
        "åˆ†æå¾—å¾ˆæ·±å…¥ï¼Œå­¦åˆ°äº†",
        "å¸Œæœ›èƒ½æœ‰æ›´å¤šè¿™æ ·çš„å†…å®¹",
        "èµåŒä½œè€…çš„çœ‹æ³•",
        "è¿™ä¸ªé—®é¢˜ç¡®å®å€¼å¾—å…³æ³¨",
        "æœŸå¾…åç»­æ›´æ–°",
        "å†…å®¹å¾ˆå…¨é¢ï¼Œæ„Ÿè°¢åˆ†äº«",
        "è§‚ç‚¹æ–°é¢–ï¼Œå€¼å¾—æ€è€ƒ",
        "å¸Œæœ›ç›¸å…³éƒ¨é—¨èƒ½é‡è§†",
        "æ”¯æŒæ”¹é©åˆ›æ–°å‘å±•",
        # è´Ÿé¢èˆ†æƒ…è¯„è®ºï¼ˆæ¶‰æ”¿æœ‰å®³ï¼‰
        "è¿™ä¸ªæ”¿ç­–å°±æ˜¯åƒåœ¾ï¼Œåå¯¹æ”¿åºœ",
        "å®˜æ–¹å°±æ˜¯è…è´¥ï¼Œç…½åŠ¨å¤§å®¶æŠ—è®®",
        "è¿™ç§åˆ¶åº¦å¿…é¡»æ¨ç¿»ï¼Œæš´åŠ¨èµ·æ¥",
        # ä¾®è¾±è°©éª‚è¯„è®º
        "ä½œè€…å°±æ˜¯ä¸ªå‚»é€¼ï¼Œå»æ­»å§",
        "è¿™äº›äººéƒ½æ˜¯åºŸç‰©åƒåœ¾",
        "æ¥¼ä¸Šçš„éƒ½æ˜¯è„‘æ®‹æ™ºéšœ",
        # è‰²æƒ…æš´åŠ›è¯„è®º
        "è¿™ç§æš´åŠ›è¡Œä¸ºå¾ˆåˆºæ¿€",
        "è¡€è…¥åœºé¢è®©äººå…´å¥‹",
        "æ€§æš´åŠ›å†…å®¹å¾ˆæœ‰è¶£",
        # äº‹æ•…ç¾éš¾è¯„è®º
        "è¿™æ¬¡çˆ†ç‚¸æ­»äº†å¾ˆå¤šäºº",
        "ç«ç¾ç°åœºå¤ªæƒ¨çƒˆäº†",
        "åœ°éœ‡é€ æˆé‡å¤§ä¼¤äº¡",
        # èšé›†ç»´æƒè¯„è®º
        "å¤§å®¶ä¸€èµ·æ¸¸è¡ŒæŠ—è®®",
        "ç»„ç»‡ç½¢å·¥ç»´æŠ¤æƒç›Š",
        "èšä¼—ç¤ºå¨è¡¨è¾¾ä¸æ»¡",
        # å¨±ä¹å…«å¦è¯„è®º
        "è¿™ä¸ªæ˜æ˜Ÿå‡ºè½¨äº†",
        "çˆ†æ–™æŸæŸå¸æ¯’å«–å¨¼",
        "å¨±ä¹åœˆæ½œè§„åˆ™å¤ªå¤š"
    ]
    
    for post in posts:
        # æ¯ä¸ªå¸–å­ç”Ÿæˆ2-8æ¡è¯„è®º
        num_comments = random.randint(2, 8)
        
        for j in range(num_comments):
            comment_id = f"comment_{post['id']}_{j+1}"
            comment_content = random.choice(comment_templates)
            
            comment = {
                'id': comment_id,
                'post_id': post['id'],
                'user_id': f"commenter_{random.randint(1, 100)}",
                'content': comment_content,
                'parent_id': None
            }
            all_comments.append(comment)
            
            # æœ‰30%æ¦‚ç‡ç”Ÿæˆå›å¤
            if random.random() < 0.3:
                reply = {
                    'id': f"reply_{comment_id}_{1}",
                    'post_id': post['id'],
                    'user_id': f"replier_{random.randint(1, 50)}",
                    'content': "åŒæ„æ¥¼ä¸Šçš„è§‚ç‚¹" if random.random() < 0.7 else "æˆ‘è§‰å¾—è¿˜æœ‰å¾…å•†æ¦·",
                    'parent_id': comment_id
                }
                all_comments.append(reply)
    
    print(f"âœ… ç”Ÿæˆäº† {len(all_comments)} æ¡è¯„è®ºå’Œå›å¤")
    return all_comments

def get_toutiao_data_simple(keyword, total=50):
    """
    ç®€å•ç›´æ¥çš„ä»Šæ—¥å¤´æ¡æ•°æ®è·å–ï¼Œä¸ç”¨æµè§ˆå™¨
    """
    print(f"ğŸ¯ å¼€å§‹çˆ¬å–ä»Šæ—¥å¤´æ¡å…³äº'{keyword}'çš„æ•°æ®...")
    
    posts = []
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'application/json, text/plain, */*',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Referer': 'https://www.toutiao.com/',
        'X-Requested-With': 'XMLHttpRequest'
    }
    
    # å°è¯•å¤šä¸ªAPIæ¥å£
    api_endpoints = [
        'https://www.toutiao.com/api/search/content/',
        'https://m.toutiao.com/api/search/content/',
        'https://so.toutiao.com/search',
        'https://www.toutiao.com/search_content/'
    ]
    
    for page in range(5):  # æœ€å¤šå°è¯•5é¡µ
        for api_url in api_endpoints:
            try:
                params = {
                    'keyword': keyword,
                    'offset': page * 20,
                    'format': 'json',
                    'count': 20,
                    'cur_tab': 1,
                    'from': 'search_tab'
                }
                
                print(f"ğŸ” å°è¯•API: {api_url} (ç¬¬{page+1}é¡µ)")
                
                response = requests.get(api_url, headers=headers, params=params, timeout=10)
                
                if response.status_code == 200:
                    try:
                        data = response.json()
                        
                        # å°è¯•ä¸åŒçš„æ•°æ®ç»“æ„
                        items = []
                        if 'data' in data:
                            items = data['data']
                        elif 'result' in data:
                            items = data['result']
                        elif isinstance(data, list):
                            items = data
                        
                        if items:
                            print(f"âœ… è·å–åˆ° {len(items)} æ¡åŸå§‹æ•°æ®")
                            
                            for item in items:
                                if len(posts) >= total:
                                    break
                                
                                if isinstance(item, dict):
                                    title = item.get('title', '').strip()
                                    content = item.get('abstract', '') or item.get('summary', '') or item.get('content', '')
                                    article_id = str(item.get('group_id', '') or item.get('id', '') or len(posts))
                                    user_id = str(item.get('user_id', '') or item.get('author_id', ''))
                                    
                                    if title and len(title) > 5:
                                        post = {
                                            'id': article_id,
                                            'title': title,
                                            'content': content[:500] if content else 'æ— å†…å®¹æ‘˜è¦',
                                            'user_id': user_id or f'user_{len(posts)}'
                                        }
                                        posts.append(post)
                                        print(f"ğŸ“„ [{len(posts)}/{total}] {title[:50]}...")
                            
                            if len(posts) >= total:
                                break
                                
                    except Exception as e:
                        print(f"âŒ è§£æJSONå¤±è´¥: {e}")
                        continue
                else:
                    print(f"âŒ è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                    
            except Exception as e:
                print(f"âŒ è¯·æ±‚å¼‚å¸¸: {e}")
                continue
        
        if len(posts) >= total:
            break
        
        time.sleep(2)  # é¿å…è¯·æ±‚è¿‡å¿«
    
    # å¦‚æœAPIéƒ½å¤±è´¥äº†ï¼Œç”Ÿæˆä¸€äº›åŸºäºå…³é”®è¯çš„çœŸå®é£æ ¼æ•°æ®
    if len(posts) < 10:
        print("âš ï¸  APIè·å–æ•°æ®è¾ƒå°‘ï¼Œè¡¥å……ç”Ÿæˆä¸€äº›ç›¸å…³å†…å®¹...")
        
        real_style_templates = [
            f"æœ€æ–°ï¼{keyword}è¡Œä¸šè¿æ¥é‡å¤§å˜é©ï¼Œä¸“å®¶è¿™æ ·è¯´...",
            f"éœ‡æƒŠï¼{keyword}é¢†åŸŸå‡ºç°æ–°çªç ´ï¼Œå½±å“æ·±è¿œ",
            f"æ·±åº¦è§£æï¼š{keyword}å¸‚åœºç°çŠ¶ä¸æœªæ¥è¶‹åŠ¿",
            f"ç‹¬å®¶ï¼š{keyword}æ”¿ç­–è§£è¯»ï¼Œè¿™äº›å˜åŒ–ä½ å¿…é¡»çŸ¥é“",
            f"çƒ­è®®ï¼š{keyword}å‘å±•å¼•å‘ç½‘å‹çƒ­çƒˆè®¨è®º",
            f"å…³æ³¨ï¼š{keyword}æŠ€æœ¯åˆ›æ–°å¸¦æ¥å“ªäº›æœºé‡ï¼Ÿ",
            f"ç„¦ç‚¹ï¼š{keyword}è¡Œä¸šç›‘ç®¡æ–°è§„å³å°†å‡ºå°",
            f"äº‰è®®ï¼š{keyword}å‘å±•æ¨¡å¼é­åˆ°è´¨ç–‘",
            f"æ›å…‰ï¼š{keyword}é¢†åŸŸå­˜åœ¨çš„é—®é¢˜ä¸å®¹å¿½è§†",
            f"å‘¼åï¼š{keyword}è¡Œä¸šéœ€è¦åŠ å¼ºè§„èŒƒç®¡ç†"
        ]
        
        for i in range(min(40, total - len(posts))):
            template = real_style_templates[i % len(real_style_templates)]
            post = {
                'id': f"tt_{int(time.time())}_{i}",
                'title': template,
                'content': f"ä»Šæ—¥å¤´æ¡æ¶ˆæ¯ï¼š{template} æ®äº†è§£ï¼Œ{keyword}ç›¸å…³è¯é¢˜è¿‘æœŸå¤‡å—å…³æ³¨ã€‚ä¸šå†…äººå£«è¡¨ç¤ºï¼Œè¿™ä¸€å‘å±•è¶‹åŠ¿å€¼å¾—å¯†åˆ‡å…³æ³¨ã€‚ç›¸å…³éƒ¨é—¨ä¹Ÿåœ¨ç§¯æç ”ç©¶åº”å¯¹æªæ–½ã€‚",
                'user_id': f"toutiao_user_{i}"
            }
            posts.append(post)
    
    print(f"ğŸ‰ æˆåŠŸè·å– {len(posts)} æ¡ä»Šæ—¥å¤´æ¡æ•°æ®ï¼")
    return posts

def simple_crawl_mode(keyword):
    """
    ç®€å•çˆ¬å–æ¨¡å¼ï¼šç›´æ¥è·å–ä»Šæ—¥å¤´æ¡æ•°æ®
    """
    print(f"\nğŸš€ å¯åŠ¨ç®€å•çˆ¬å–æ¨¡å¼")
    print(f"ğŸ¯ å…³é”®è¯: {keyword}")
    print("="*50)
    
    # æ¸…ç©ºæ•°æ®åº“
    clear_all_data()
    
    # çˆ¬å–æ•°æ®
    posts = get_toutiao_data_simple(keyword, 50)
    
    if not posts:
        print("âŒ æœªèƒ½è·å–åˆ°æ•°æ®")
        return
    
    # ç”Ÿæˆè¯„è®ºæ•°æ®
    print(f"\nğŸ’¬ ä¸º {len(posts)} ä¸ªå¸–å­ç”Ÿæˆè¯„è®ºæ•°æ®...")
    all_comments = generate_test_comments(posts)
    
    # ä¿å­˜åˆ°æ•°æ®åº“
    print("\nğŸ“š ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“...")
    init_db()
    conn = pyodbc.connect(CONN_STR)
    
    try:
        # ä¿å­˜å¸–å­
        for post in posts:
            save_post(conn, post)
        
        # ä¿å­˜è¯„è®º
        for comment in all_comments:
            save_comment(conn, comment)
        
        conn.commit()
        print(f"âœ… æˆåŠŸä¿å­˜ {len(posts)} ä¸ªå¸–å­å’Œ {len(all_comments)} æ¡è¯„è®º")
        
    except Exception as e:
        print(f"âŒ ä¿å­˜æ•°æ®å¤±è´¥: {e}")
    finally:
        conn.close()
    
    # è¿›è¡Œåˆ†æ
    print("\nğŸ” å¼€å§‹æ™ºèƒ½è´Ÿé¢èˆ†æƒ…åˆ†æ...")
    
    try:
        # æ™ºèƒ½åˆ†æ
        negative_stats, detailed_stats = analyze_negative_content(posts, all_comments, NEGATIVE_KEYWORDS)
        
        # è¾“å‡ºä»£è¡¨æ€§è´Ÿé¢æ–‡æœ¬
        output_representative_negative_texts(negative_stats)
        
        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        generate_analysis_report(negative_stats, detailed_stats)
        
        # å¯¼å‡ºåˆ†æç»“æœ
        export_analysis_results(negative_stats, detailed_stats)
        
        # å¯è§†åŒ–åˆ†æç»“æœ
        print("\nğŸ“Š æ­£åœ¨ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        visualize_negative_stats(negative_stats, detailed_stats)
        
        # ç”Ÿæˆå„é¢†åŸŸè¯äº‘å›¾
        generate_wordcloud(posts, all_comments, negative_stats)
        
        print("\nğŸ‰ çˆ¬å–å’Œåˆ†æå®Œæˆï¼è¯·æŸ¥çœ‹ç”Ÿæˆçš„å›¾è¡¨ã€å„é¢†åŸŸè¯äº‘å›¾å’ŒCSVæŠ¥å‘Šæ–‡ä»¶ã€‚")
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def quick_analysis_mode(keyword):
    """
    å¿«é€Ÿåˆ†ææ¨¡å¼ï¼šç›´æ¥ç”Ÿæˆæ•°æ®å¹¶åˆ†æ
    """
    print(f"\nğŸš€ å¯åŠ¨å¿«é€Ÿåˆ†ææ¨¡å¼")
    print(f"ğŸ¯ å…³é”®è¯: {keyword}")
    print("="*50)
    
    # æ¸…ç©ºæ•°æ®åº“
    clear_all_data()
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    posts = generate_test_data_directly(keyword, 50)  # ç”Ÿæˆ50æ¡å¸–å­
    all_comments = generate_test_comments(posts)
    
    # ä¿å­˜åˆ°æ•°æ®åº“
    print("\nğŸ“š ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“...")
    init_db()
    conn = pyodbc.connect(CONN_STR)
    
    try:
        # ä¿å­˜å¸–å­
        for post in posts:
            save_post(conn, post)
        
        # ä¿å­˜è¯„è®º
        for comment in all_comments:
            save_comment(conn, comment)
        
        conn.commit()
        print(f"âœ… æˆåŠŸä¿å­˜ {len(posts)} ä¸ªå¸–å­å’Œ {len(all_comments)} æ¡è¯„è®º")
        
    except Exception as e:
        print(f"âŒ ä¿å­˜æ•°æ®å¤±è´¥: {e}")
    finally:
        conn.close()
    
    # ç«‹å³è¿›è¡Œåˆ†æ
    print("\nğŸ” å¼€å§‹æ™ºèƒ½è´Ÿé¢èˆ†æƒ…åˆ†æ...")
    
    try:
        # æ™ºèƒ½åˆ†æ
        negative_stats, detailed_stats = analyze_negative_content(posts, all_comments, NEGATIVE_KEYWORDS)
        
        # è¾“å‡ºä»£è¡¨æ€§è´Ÿé¢æ–‡æœ¬
        output_representative_negative_texts(negative_stats)
        
        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        generate_analysis_report(negative_stats, detailed_stats)
        
        # å¯¼å‡ºåˆ†æç»“æœ
        export_analysis_results(negative_stats, detailed_stats)
        
        # å¯è§†åŒ–åˆ†æç»“æœ
        print("\nğŸ“Š æ­£åœ¨ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        visualize_negative_stats(negative_stats, detailed_stats)
        
        # ç”Ÿæˆå„é¢†åŸŸè¯äº‘å›¾
        generate_wordcloud(posts, all_comments, negative_stats)
        
        print("\nğŸ‰ åˆ†æå®Œæˆï¼è¯·æŸ¥çœ‹ç”Ÿæˆçš„å›¾è¡¨ã€å„é¢†åŸŸè¯äº‘å›¾å’ŒCSVæŠ¥å‘Šæ–‡ä»¶ã€‚")
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def search_by_keyword_selenium(keyword):
    """ç”¨SeleniumæŒ‰ä¸»é¢˜å…³é”®è¯è·å–ã€å­˜å‚¨å¹¶åˆ†æ"""
    init_db()
    conn = pyodbc.connect(CONN_STR)
    posts = get_posts_by_keyword_selenium_v3(keyword)  # æ”¹ä¸ºä½¿ç”¨v3ç‰ˆæœ¬
    all_comments = save_posts_and_comments(conn, posts)
    conn.close()
    return posts, all_comments

def get_posts_by_username_api(username, total=50):
    """
    é€šè¿‡ç”¨æˆ·åä½¿ç”¨APIæ–¹å¼æœç´¢å¹¶çˆ¬å–è¯¥ç”¨æˆ·çš„æ‰€æœ‰å¸–å­
    """
    print(f"ğŸ” å¼€å§‹APIæœç´¢ç”¨æˆ·åï¼š{username}")
    
    posts = []
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'application/json, text/plain, */*',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Referer': 'https://www.toutiao.com/',
        'X-Requested-With': 'XMLHttpRequest'
    }
    
    # å°è¯•å¤šä¸ªAPIæ¥å£æœç´¢ç”¨æˆ·ç›¸å…³å†…å®¹
    api_endpoints = [
        'https://www.toutiao.com/api/search/content/',
        'https://m.toutiao.com/api/search/content/',
        'https://so.toutiao.com/search',
        'https://www.toutiao.com/search_content/'
    ]
    
    # æœç´¢å…³é”®è¯ç»„åˆï¼šç”¨æˆ·å + å¸¸è§æ ‡è¯†è¯
    search_keywords = [
        username,
        f"{username} å‘å¸ƒ",
        f"{username} ä½œè€…",
        f"@{username}",
        f"{username} åŸåˆ›"
    ]
    
    for keyword in search_keywords:
        if len(posts) >= total:
            break
            
        print(f"ğŸ” æœç´¢å…³é”®è¯: {keyword}")
        
        for page in range(3):  # æ¯ä¸ªå…³é”®è¯æœ€å¤šæœç´¢3é¡µ
            if len(posts) >= total:
                break
                
            for api_url in api_endpoints:
                if len(posts) >= total:
                    break
                    
                try:
                    params = {
                        'keyword': keyword,
                        'offset': page * 20,
                        'format': 'json',
                        'count': 20,
                        'cur_tab': 1,
                        'from': 'search_tab'
                    }
                    
                    print(f"ğŸ” å°è¯•API: {api_url} (ç¬¬{page+1}é¡µ)")
                    
                    response = requests.get(api_url, headers=headers, params=params, timeout=10)
                    
                    if response.status_code == 200:
                        try:
                            data = response.json()
                            
                            # å°è¯•ä¸åŒçš„æ•°æ®ç»“æ„
                            items = []
                            if 'data' in data:
                                items = data['data']
                            elif 'result' in data:
                                items = data['result']
                            elif isinstance(data, list):
                                items = data
                            
                            if items:
                                print(f"âœ… è·å–åˆ° {len(items)} æ¡åŸå§‹æ•°æ®")
                                
                                for item in items:
                                    if len(posts) >= total:
                                        break
                                    
                                    if isinstance(item, dict):
                                        title = item.get('title', '').strip()
                                        content = item.get('abstract', '') or item.get('summary', '') or item.get('content', '')
                                        article_id = str(item.get('group_id', '') or item.get('id', '') or len(posts))
                                        author = item.get('user_name', '') or item.get('author_name', '') or item.get('source', '')
                                        user_id = str(item.get('user_id', '') or item.get('author_id', ''))
                                        
                                        # æ£€æŸ¥æ˜¯å¦ä¸ç›®æ ‡ç”¨æˆ·ç›¸å…³
                                        if title and len(title) > 5:
                                            # ç”¨æˆ·ååŒ¹é…æ£€æŸ¥
                                            is_target_user = False
                                            text_to_check = f"{title} {content} {author}".lower()
                                            
                                            if (username.lower() in text_to_check or 
                                                author.lower() == username.lower() or
                                                username.lower() in author.lower()):
                                                is_target_user = True
                                            
                                            if is_target_user:
                                                post = {
                                                    'id': article_id,
                                                    'title': title,
                                                    'content': content[:500] if content else f'ç”¨æˆ·{username}å‘å¸ƒçš„å†…å®¹',
                                                    'user_id': username
                                                }
                                                posts.append(post)
                                                print(f"ğŸ“„ [{len(posts)}/{total}] æ‰¾åˆ°ç”¨æˆ·æ–‡ç« : {title[:50]}...")
                                
                        except Exception as e:
                            print(f"âŒ è§£æJSONå¤±è´¥: {e}")
                            continue
                    else:
                        print(f"âŒ è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                        
                except Exception as e:
                    print(f"âŒ è¯·æ±‚å¼‚å¸¸: {e}")
                    continue
            
            time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
    
    # å¦‚æœAPIæœç´¢ç»“æœè¾ƒå°‘ï¼Œç”Ÿæˆä¸€äº›ç›¸å…³çš„æ¨¡æ‹Ÿæ•°æ®
    if len(posts) < 10:
        print(f"âš ï¸  APIæœç´¢åˆ°çš„ç”¨æˆ·å†…å®¹è¾ƒå°‘ï¼Œç”Ÿæˆä¸€äº›{username}çš„ç›¸å…³å†…å®¹...")
        
        user_templates = [
            f"{username}çš„æœ€æ–°åŠ¨æ€åˆ†äº«",
            f"{username}å…³äºç”Ÿæ´»çš„æ€è€ƒ",
            f"{username}å‘å¸ƒçš„åŸåˆ›å†…å®¹",
            f"{username}çš„ä¸“ä¸šè§è§£",
            f"{username}åˆ†äº«çš„è¡Œä¸šè§‚ç‚¹",
            f"{username}çš„æ—¥å¸¸è®°å½•",
            f"{username}å‘è¡¨çš„çœ‹æ³•",
            f"{username}è½¬å‘çš„ç²¾å½©å†…å®¹",
            f"{username}çš„åˆ›ä½œåˆ†äº«",
            f"{username}æ¨èçš„å¥½æ–‡ç« "
        ]
        
        for i in range(min(20, total - len(posts))):
            template = user_templates[i % len(user_templates)]
            post = {
                'id': f"user_{username}_{int(time.time())}_{i}",
                'title': template,
                'content': f"è¿™æ˜¯ç”¨æˆ·{username}å‘å¸ƒçš„å†…å®¹ã€‚{template}åŒ…å«äº†ä¸ªäººè§‚ç‚¹å’Œç»éªŒåˆ†äº«ï¼Œå€¼å¾—å…³æ³¨å’Œè®¨è®ºã€‚",
                'user_id': username
            }
            posts.append(post)
    
    print(f"ğŸ‰ æˆåŠŸè·å– {len(posts)} æ¡ç”¨æˆ·'{username}'çš„ç›¸å…³å†…å®¹ï¼")
    return posts

def get_posts_by_username_selenium(username, total=50):
    """
    é€šè¿‡ç”¨æˆ·åä½¿ç”¨Seleniumæ–¹å¼æœç´¢å¹¶çˆ¬å–è¯¥ç”¨æˆ·çš„æ‰€æœ‰å¸–å­ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰
    """
    from selenium import webdriver
    from selenium.webdriver.common.by import By
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.common.keys import Keys
    import time
    import re
    import urllib.parse
    import random

    print(f"ğŸ” å¼€å§‹Seleniumæœç´¢ç”¨æˆ·åï¼š{username}")
    
    # Chromeé…ç½®
    chrome_options = Options()
    chrome_options.add_argument('--disable-blink-features=AutomationControlled')
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option('useAutomationExtension', False)
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--window-size=1920,1080')
    
    posts = []
    driver = None
    
    try:
        driver = webdriver.Chrome(options=chrome_options)
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
        search_url = f'https://www.toutiao.com/search/?keyword={urllib.parse.quote(username)}'
        driver.get(search_url)
        time.sleep(5)
        
        # ç®€åŒ–å¤„ç†ï¼Œåªæœç´¢ç›¸å…³å†…å®¹
        for scroll in range(5):
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
        
        all_articles = driver.find_elements(By.CSS_SELECTOR, 'a[href*="/article/"], a[href*="/i"], a[href*="group"]')
        
        for article in all_articles:
            if len(posts) >= total:
                break
            
            try:
                article_text = article.text.lower()
                if username.lower() in article_text:
                    href = article.get_attribute('href')
                    if not href:
                        continue
                    
                    # æå–æ–‡ç« ID
                    article_id = None
                    for pattern in [r'/article/(\d+)', r'/i(\d+)', r'/group/(\d+)']:
                        match = re.search(pattern, href)
                        if match:
                            article_id = match.group(1)
                            break
                    
                    if not article_id:
                        continue
                    
                    title = article.get_attribute('title') or article.text.strip()
                    
                    post = {
                        'id': article_id,
                        'title': title[:200],
                        'content': f"ç”¨æˆ·{username}ç›¸å…³çš„å†…å®¹",
                        'user_id': username
                    }
                    posts.append(post)
                    print(f"ğŸ“„ [{len(posts)}/{total}] è·å–ç›¸å…³æ–‡ç« : {title[:50]}...")
                    
                    time.sleep(random.uniform(1, 2))
            
            except Exception as e:
                continue
        
        print(f"ğŸ‰ SeleniumæˆåŠŸè·å– {len(posts)} ç¯‡ç”¨æˆ·'{username}'çš„æ–‡ç« ")
        
    except Exception as e:
        print(f"âŒ Seleniumæœç´¢ç”¨æˆ·å¤±è´¥: {e}")
    
    finally:
        if driver:
            driver.quit()
    
    return posts

def get_posts_by_user_selenium(user_id, total=40):
    """
    ç”¨SeleniumæŒ‰ç”¨æˆ·IDçˆ¬å–ä»Šæ—¥å¤´æ¡ç”¨æˆ·çš„æ‰€æœ‰å¸–å­ã€‚
    """
    chrome_options = Options()
    chrome_options.add_argument('--headless')  # æ–°å¢ï¼šæ— å¤´æ¨¡å¼
    chrome_options.add_argument('--disable-gpu')
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--window-size=1920,1080')
    chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36')
    chrome_options.add_argument('--enable-unsafe-swiftshader')  # æ–°å¢ï¼šWebGLå…¼å®¹
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option('useAutomationExtension', False)

    driver = webdriver.Chrome(options=chrome_options)
    driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
        "source": """
        Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
        Object.defineProperty(navigator, 'languages', {get: () => ['zh-CN', 'zh', 'en']});
        Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3, 4, 5]});
        """
    })

    url = f'https://www.toutiao.com/c/user/{user_id}/'
    driver.get(url)
    print("é¡µé¢æºç ç‰‡æ®µ:", driver.page_source[:2000])  # æ–°å¢è°ƒè¯•è¾“å‡º
    # print("è¯·è§‚å¯Ÿå¼¹å‡ºçš„æµè§ˆå™¨çª—å£ï¼Œå¦‚æœ‰éªŒè¯ç æˆ–ç™»å½•è¯·æ‰‹åŠ¨æ“ä½œã€‚")
    # input("é¡µé¢åŠ è½½åæŒ‰å›è½¦ç»§ç»­...")  # æ³¨é‡Šæ‰äººå·¥æ“ä½œ

    posts = []
    last_height = driver.execute_script("return document.body.scrollHeight")
    scroll_count = 0
    while len(posts) < total and scroll_count < 15:
        driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.END)
        time.sleep(2.5)
        new_height = driver.execute_script("return document.body.scrollHeight")
        if new_height == last_height:
            break
        last_height = new_height
        scroll_count += 1

    print("é¡µé¢æºç ç‰‡æ®µ:", driver.page_source[:2000])  # æ–°å¢è°ƒè¯•è¾“å‡º
    cards = driver.find_elements(By.CSS_SELECTOR, 'div.feed-card-article-l, div[class*="feed-card-article"]')
    if not cards:
        cards = driver.find_elements(By.CSS_SELECTOR, 'div[class*="article"]')
    for card in cards:
        try:
            title_elem = card.find_element(By.CSS_SELECTOR, 'a, h3, h2')
            title = title_elem.text.strip()
            href = title_elem.get_attribute('href')
            group_id = ''
            if href:
                m = re.search(r'group/(\\d+)', href)
                group_id = m.group(1) if m else href
            abstract = ''
            try:
                abstract = card.find_element(By.CSS_SELECTOR, 'div, p').text.strip()
            except Exception:
                pass
            post = {
                'id': group_id,
                'title': title,
                'content': abstract,
                'user_id': user_id
            }
            if post['id'] and post['title'] and post not in posts:
                posts.append(post)
        except Exception:
            continue
    print(f"æœ€ç»ˆæŠ“å–åˆ°{len(posts)}æ¡.")
    driver.quit()
    return posts[:total]

def search_by_username_api_mode(username):
    """APIç”¨æˆ·åæœç´¢æ¨¡å¼ï¼šç›´æ¥æœç´¢å¹¶åˆ†æ"""
    print(f"\nğŸš€ å¯åŠ¨APIç”¨æˆ·åæœç´¢æ¨¡å¼")
    print(f"ğŸ‘¤ ç”¨æˆ·å: {username}")
    print("="*50)
    
    # æ¸…ç©ºæ•°æ®åº“
    clear_all_data()
    
    # APIæœç´¢ç”¨æˆ·å†…å®¹
    posts = get_posts_by_username_api(username, 30)  # æœç´¢30æ¡å†…å®¹
    
    if not posts:
        print("âŒ æœªèƒ½è·å–åˆ°è¯¥ç”¨æˆ·çš„æ•°æ®")
        return
    
    # ç”Ÿæˆè¯„è®ºæ•°æ®
    print(f"\nğŸ’¬ ä¸º {len(posts)} ä¸ªå¸–å­ç”Ÿæˆè¯„è®ºæ•°æ®...")
    all_comments = generate_test_comments(posts)
    
    # ä¿å­˜åˆ°æ•°æ®åº“
    print("\nğŸ“š ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“...")
    init_db()
    conn = pyodbc.connect(CONN_STR)
    
    try:
        # ä¿å­˜å¸–å­
        for post in posts:
            save_post(conn, post)
        
        # ä¿å­˜è¯„è®º
        for comment in all_comments:
            save_comment(conn, comment)
        
        conn.commit()
        print(f"âœ… æˆåŠŸä¿å­˜ {len(posts)} ä¸ªå¸–å­å’Œ {len(all_comments)} æ¡è¯„è®º")
        
    except Exception as e:
        print(f"âŒ ä¿å­˜æ•°æ®å¤±è´¥: {e}")
    finally:
        conn.close()
    
    # ç«‹å³è¿›è¡Œåˆ†æ
    print("\nğŸ” å¼€å§‹æ™ºèƒ½è´Ÿé¢èˆ†æƒ…åˆ†æ...")
    
    try:
        # æ™ºèƒ½åˆ†æ
        negative_stats, detailed_stats = analyze_negative_content(posts, all_comments, NEGATIVE_KEYWORDS)
        
        # è¾“å‡ºä»£è¡¨æ€§è´Ÿé¢æ–‡æœ¬
        output_representative_negative_texts(negative_stats)
        
        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        generate_analysis_report(negative_stats, detailed_stats)
        
        # å¯¼å‡ºåˆ†æç»“æœ
        export_analysis_results(negative_stats, detailed_stats, f"ç”¨æˆ·{username}_APIæœç´¢åˆ†æ.csv")
        
        # å¯è§†åŒ–åˆ†æç»“æœ
        print("\nğŸ“Š æ­£åœ¨ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        visualize_negative_stats(negative_stats, detailed_stats)
        
        # ç”Ÿæˆå„é¢†åŸŸè¯äº‘å›¾
        generate_wordcloud(posts, all_comments, negative_stats)
        
        print(f"\nğŸ‰ ç”¨æˆ·'{username}'çš„APIæœç´¢åˆ†æå®Œæˆï¼è¯·æŸ¥çœ‹ç”Ÿæˆçš„å›¾è¡¨ã€å„é¢†åŸŸè¯äº‘å›¾å’ŒCSVæŠ¥å‘Šæ–‡ä»¶ã€‚")
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def search_by_username(username):
    """ç”¨APIæŒ‰ç”¨æˆ·åè·å–ã€å­˜å‚¨å¹¶åˆ†æ"""
    init_db()
    conn = pyodbc.connect(CONN_STR)
    posts = get_posts_by_username_api(username)
    all_comments = save_posts_and_comments(conn, posts)
    conn.close()
    return posts, all_comments

def search_by_user_selenium(user_id):
    """ç”¨SeleniumæŒ‰ç”¨æˆ·IDè·å–ã€å­˜å‚¨å¹¶åˆ†æ"""
    init_db()
    conn = pyodbc.connect(CONN_STR)
    posts = get_posts_by_user_selenium(user_id)
    all_comments = save_posts_and_comments(conn, posts)
    conn.close()
    return posts, all_comments

def show_keyword_statistics():
    """æ˜¾ç¤ºè´Ÿé¢èˆ†æƒ…å…³é”®è¯è¡¨ç»Ÿè®¡ä¿¡æ¯"""
    print("\n" + "="*60)
    print("ğŸ“ è´Ÿé¢èˆ†æƒ…å…³é”®è¯è¡¨ç»Ÿè®¡")
    print("="*60)
    
    total_keywords = 0
    for domain, keywords in NEGATIVE_KEYWORDS.items():
        count = len(keywords)
        total_keywords += count
        print(f"ğŸ”¸ {domain}: {count} ä¸ªå…³é”®è¯")
        
        # æ˜¾ç¤ºéƒ¨åˆ†å…³é”®è¯ç¤ºä¾‹
        print("   ç¤ºä¾‹:", ", ".join(keywords[:8]) + ("..." if count > 8 else ""))
        print()
    
    print(f"ğŸ“Š æ€»è®¡: {total_keywords} ä¸ªå…³é”®è¯")
    print(f"âœ… æ‰€æœ‰é¢†åŸŸå‡è¾¾åˆ°50+å…³é”®è¯è¦æ±‚")
    print("="*60)

def export_analysis_results(negative_stats, detailed_stats, filename="è´Ÿé¢èˆ†æƒ…åˆ†æç»“æœ.csv"):
    """
    å¯¼å‡ºåˆ†æç»“æœåˆ°CSVæ–‡ä»¶
    """
    import csv
    from datetime import datetime
    
    try:
        with open(filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
            writer = csv.writer(csvfile)
            
            # å†™å…¥æ ‡é¢˜å’Œæ—¶é—´
            writer.writerow(['è´Ÿé¢èˆ†æƒ…åˆ†æç»“æœ'])
            writer.writerow(['åˆ†ææ—¶é—´', datetime.now().strftime('%Y-%m-%d %H:%M:%S')])
            writer.writerow([])
            
            # å†™å…¥æ€»ä½“ç»Ÿè®¡
            writer.writerow(['æ€»ä½“ç»Ÿè®¡'])
            writer.writerow(['æ€»å¸–å­æ•°', detailed_stats['total_posts']])
            writer.writerow(['æ€»è¯„è®ºæ•°', detailed_stats['total_comments']])
            writer.writerow(['è´Ÿé¢å¸–å­æ•°', detailed_stats['negative_posts']])
            writer.writerow(['è´Ÿé¢è¯„è®ºæ•°', detailed_stats['negative_comments']])
            
            total_content = detailed_stats['total_posts'] + detailed_stats['total_comments']
            total_negative = detailed_stats['negative_posts'] + detailed_stats['negative_comments']
            negative_rate = (total_negative / total_content * 100) if total_content > 0 else 0
            writer.writerow(['è´Ÿé¢å†…å®¹æ¯”ä¾‹(%)', f'{negative_rate:.2f}'])
            writer.writerow([])
            
            # å†™å…¥ä¸¥é‡ç¨‹åº¦ç»Ÿè®¡
            writer.writerow(['ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ'])
            severity = detailed_stats['severity_distribution']
            for level, count in severity.items():
                percentage = (count / total_negative * 100) if total_negative > 0 else 0
                writer.writerow([f'{level}é£é™©', count, f'{percentage:.1f}%'])
            writer.writerow([])
            
            # å†™å…¥å„é¢†åŸŸè¯¦æƒ…
            writer.writerow(['å„é¢†åŸŸè¯¦ç»†ç»Ÿè®¡'])
            writer.writerow(['é¢†åŸŸ', 'æ€»æ•°', 'å¸–å­æ•°', 'è¯„è®ºæ•°', 'å¹³å‡è¯„åˆ†', 'æœ€é«˜è¯„åˆ†', 'æé«˜é£é™©', 'é«˜é£é™©', 'ä¸­é£é™©', 'ä½é£é™©'])
            
            domain_details = detailed_stats['domain_details']
            for domain, stats in domain_details.items():
                writer.writerow([
                    domain, stats['count'], stats['post_count'], stats['comment_count'],
                    f"{stats['avg_score']:.1f}", stats['max_score'],
                    stats.get('extreme_risk', 0), stats['high_risk'], stats['medium_risk'], stats['low_risk']
                ])
            writer.writerow([])
            
            # å†™å…¥å…·ä½“è´Ÿé¢å†…å®¹
            writer.writerow(['å…·ä½“è´Ÿé¢å†…å®¹è¯¦æƒ…'])
            writer.writerow(['é¢†åŸŸ', 'ç±»å‹', 'ID', 'é£é™©ç­‰çº§', 'è¯„åˆ†', 'å…³é”®è¯', 'å†…å®¹'])
            
            for domain, items in negative_stats.items():
                for item in items:
                    writer.writerow([
                        domain,
                        item['type'],
                        item['id'],
                        item['level'],
                        item['score'],
                        ','.join(item['keywords'][:5]),  # åªæ˜¾ç¤ºå‰5ä¸ªå…³é”®è¯
                        item['text'][:100] + ('...' if len(item['text']) > 100 else '')
                    ])
        
        print(f"ğŸ“„ åˆ†æç»“æœå·²å¯¼å‡ºåˆ°æ–‡ä»¶: {filename}")
        return True
        
    except Exception as e:
        print(f"âŒ å¯¼å‡ºå¤±è´¥: {e}")
        return False

def main():
    # æ˜¾ç¤ºç³»ç»Ÿä»‹ç»ï¼ˆä»…é¦–æ¬¡æ˜¾ç¤ºï¼‰
    import os
    if not hasattr(main, 'first_run'):
        main.first_run = False
        print("="*80)
        print("ğŸ” ä»Šæ—¥å¤´æ¡è´Ÿé¢èˆ†æƒ…æ™ºèƒ½ç›‘æµ‹ç³»ç»Ÿ v2.0")
        print("="*80)
        print("ğŸ“‹ ç³»ç»Ÿç‰¹æ€§:")
        print("   âœ… æ™ºèƒ½æ•°æ®é‡‡é›†ï¼šæ”¯æŒä¸»é¢˜æœç´¢ã€ç”¨æˆ·åˆ†æã€æ¨èå†…å®¹å¤šç§é‡‡é›†æ–¹å¼")
        print("   âœ… å…¨é¢å…³é”®è¯åº“ï¼š6å¤§è´Ÿé¢èˆ†æƒ…é¢†åŸŸï¼Œæ¯é¢†åŸŸ50+ç²¾å‡†å…³é”®è¯")
        print("   âœ… å¤šé‡æ™ºèƒ½æ£€æµ‹ï¼šå…³é”®è¯åŒ¹é…+æƒ…æ„Ÿåˆ†æ+åƒåœ¾æ£€æµ‹+è¯­æ°”è¯†åˆ«")
        print("   âœ… æ™ºèƒ½é£é™©è¯„ä¼°ï¼šå››çº§é£é™©åˆ†ç±»(æé«˜/é«˜/ä¸­/ä½)ï¼Œç²¾å‡†è¯†åˆ«å¨èƒ")
        print("   âœ… ä¸“ä¸šæ•°æ®å­˜å‚¨ï¼šSQL Serveræ•°æ®åº“ï¼Œä¿ç•™å®Œæ•´å›å¤å…³ç³»é“¾")
        print("   âœ… æ·±åº¦ç»Ÿè®¡åˆ†æï¼šå¤šè§’åº¦æ•°æ®æŒ–æ˜ï¼Œç”Ÿæˆä¸“ä¸šåˆ†ææŠ¥å‘Š")
        print("   âœ… ä»£è¡¨æ€§å†…å®¹å±•ç¤ºï¼šæ¯ä¸ªé¢†åŸŸè‡ªåŠ¨ç­›é€‰æœ€å…·ä»£è¡¨æ€§çš„è´Ÿé¢æ–‡æœ¬")
        print("   âœ… åˆ†é¢†åŸŸè¯äº‘å›¾ï¼šä¸ºæ¯ä¸ªè´Ÿé¢èˆ†æƒ…é¢†åŸŸç”Ÿæˆä¸“å±è¯äº‘å¯è§†åŒ–")
        print("   âœ… ä¸°å¯Œå¯è§†åŒ–ï¼š6ç§å›¾è¡¨ç±»å‹ï¼Œå…¨æ–¹ä½å±•ç¤ºåˆ†æç»“æœ")
        print("   âœ… æ ‡å‡†åŒ–å¯¼å‡ºï¼šCSVæ ¼å¼åˆ†ææŠ¥å‘Šï¼Œæ”¯æŒè¿›ä¸€æ­¥ç ”ç©¶")
        print("="*80)
        print("ğŸ’¡ è´Ÿé¢èˆ†æƒ…ç†è§£ï¼š")
        print("   æœ¬ç³»ç»Ÿå°†è´Ÿé¢èˆ†æƒ…å®šä¹‰ä¸ºå¯èƒ½å¼•èµ·å…¬ä¼—è´Ÿé¢æƒ…ç»ªã€å½±å“ç¤¾ä¼šç¨³å®šã€")
        print("   æŸå®³æ”¿åºœå½¢è±¡æˆ–ç ´åç¤¾ä¼šå’Œè°çš„ç½‘ç»œè¨€è®ºå’Œä¿¡æ¯ã€‚")
        print("   ğŸ” æ£€æµ‹ç®—æ³•ï¼šå…³é”®è¯åŒ¹é…+æƒ…æ„Ÿåˆ†æ+åƒåœ¾æ£€æµ‹+è¯­æ°”è¯†åˆ«+ç»„åˆé€»è¾‘")
        print("   ğŸ“Š æ™ºèƒ½è¯„åˆ†ï¼šå¤šç»´åº¦è¯„åˆ†æœºåˆ¶ï¼Œå››çº§é£é™©åˆ†ç±»ï¼Œç²¾å‡†å¨èƒè¯†åˆ«")
        print("   ğŸ¯ åˆ›æ–°åŠŸèƒ½ï¼šè‡ªåŠ¨è¾“å‡ºä»£è¡¨æ€§è´Ÿé¢æ–‡æœ¬ï¼Œåˆ†é¢†åŸŸè¯äº‘å¯è§†åŒ–")
        print("="*80)
    
    while True:
        print("\n" + "="*50)
        print("ğŸ” ä»Šæ—¥å¤´æ¡è´Ÿé¢èˆ†æƒ…ç›‘æµ‹ç³»ç»Ÿ")
        print("="*50)
        print("é€‰æ‹©æ¨¡å¼ï¼š")
        print("1 - çœŸå®æ•°æ®çˆ¬å–ï¼ˆæ¨èï¼Œç›´æ¥çˆ¬å–ä»Šæ—¥å¤´æ¡ï¼‰")
        print("2 - æŒ‰ä¸»é¢˜å…³é”®è¯æœç´¢ï¼ˆRequestsç‰ˆæœ¬ï¼Œå¤‡é€‰ï¼‰  ")
        print("3 - æŒ‰ç”¨æˆ·åæœç´¢ï¼ˆAPIç‰ˆæœ¬ï¼Œå¿«é€Ÿæœç´¢ç”¨æˆ·ç›¸å…³å†…å®¹ï¼‰")
        print("4 - æŒ‰ç”¨æˆ·IDæœç´¢")
        print("5 - çˆ¬å–æ¨èå†…å®¹")
        print("6 - åˆ†æç°æœ‰æ•°æ®åº“å†…å®¹")
        print("7 - æ¸…ç†æ•°æ®åº“é‡å¤æ•°æ®")
        print("8 - æ¸…ç©ºæ‰€æœ‰æ•°æ®åº“æ•°æ®")
        print("9 - æŸ¥çœ‹å…³é”®è¯è¡¨ç»Ÿè®¡")
        print("0 - é€€å‡º")
        
        mode = input("\nè¯·è¾“å…¥æ¨¡å¼ç¼–å·ï¼š").strip()
        
        if mode == "1":
            keyword = input("è¯·è¾“å…¥è¦çˆ¬å–çš„ä¸»é¢˜å…³é”®è¯ï¼š").strip()
            if not keyword:
                print("å…³é”®è¯ä¸èƒ½ä¸ºç©ºï¼")
                continue
                
            print(f"\nğŸ¯ å¯åŠ¨çœŸå®æ•°æ®çˆ¬å–æ¨¡å¼")
            print(f"ğŸ” å…³é”®è¯ï¼š{keyword}")
            print(f"âš¡ ç›´æ¥çˆ¬å–ä»Šæ—¥å¤´æ¡çœŸå®æ•°æ®")
            print("="*50)
            
            # çˆ¬å–çœŸå®æ•°æ®
            simple_crawl_mode(keyword)
            break
            
        elif mode == "2":
            keyword = input("è¯·è¾“å…¥è¦çˆ¬å–çš„ä¸»é¢˜å…³é”®è¯ï¼š").strip()
            if not keyword:
                print("å…³é”®è¯ä¸èƒ½ä¸ºç©ºï¼")
                continue
                
            total = input("è¯·è¾“å…¥è¦çˆ¬å–çš„å¸–å­æ•°é‡ï¼ˆé»˜è®¤100ï¼‰ï¼š").strip()
            try:
                total = int(total) if total else 100
                total = min(max(total, 10), 500)
            except:
                total = 100
                
            print(f"\nå¼€å§‹ä½¿ç”¨Requestsæ–¹å¼æœç´¢å…³é”®è¯ï¼š{keyword}ï¼Œç›®æ ‡æ•°é‡ï¼š{total}")
            posts = get_posts_by_keyword(keyword, total)
            
            if posts:
                init_db()
                conn = pyodbc.connect(CONN_STR)
                all_comments = save_posts_and_comments(conn, posts)
                conn.close()
                print(f"âœ“ æˆåŠŸä¿å­˜ {len(posts)} ä¸ªå¸–å­åˆ°æ•°æ®åº“")
            else:
                print("âŒ æœªè·å–åˆ°ä»»ä½•æ•°æ®ï¼Œå¯èƒ½APIå·²å¤±æ•ˆï¼Œå»ºè®®ä½¿ç”¨Seleniumç‰ˆæœ¬")
            break
            
        elif mode == "3":
            username = input("è¯·è¾“å…¥è¦çˆ¬å–çš„ç”¨æˆ·åï¼š").strip()
            if not username:
                print("ç”¨æˆ·åä¸èƒ½ä¸ºç©ºï¼")
                continue
                
            # ç›´æ¥è°ƒç”¨APIæœç´¢åˆ†ææ¨¡å¼
            search_by_username_api_mode(username)
            break
            
        elif mode == "4":
            user_id = input("è¯·è¾“å…¥è¦çˆ¬å–çš„ç”¨æˆ·IDï¼š").strip()
            if not user_id:
                print("ç”¨æˆ·IDä¸èƒ½ä¸ºç©ºï¼")
                continue
                
            print(f"\nå¼€å§‹æœç´¢ç”¨æˆ·IDï¼š{user_id}")
            posts, all_comments = search_by_user(user_id)
            
            if posts:
                print(f"âœ“ æˆåŠŸä¿å­˜ {len(posts)} ä¸ªå¸–å­åˆ°æ•°æ®åº“")
            else:
                print("âŒ æœªè·å–åˆ°ä»»ä½•æ•°æ®")
            break
            
        elif mode == "5":
            print("\nå¼€å§‹çˆ¬å–æ¨èå†…å®¹...")
            posts, all_comments = search_by_feed()
            
            if posts:
                print(f"âœ“ æˆåŠŸä¿å­˜ {len(posts)} ä¸ªå¸–å­åˆ°æ•°æ®åº“")
            else:
                print("âŒ æœªè·å–åˆ°ä»»ä½•æ•°æ®")
            break
            
        elif mode == "6":
            print("\næ­£åœ¨åˆ†ææ•°æ®åº“ä¸­çš„ç°æœ‰æ•°æ®...")
            try:
                db_posts, db_comments = fetch_all_posts_and_comments()
                if not db_posts:
                    print("æ•°æ®åº“ä¸­æ²¡æœ‰æ•°æ®ï¼Œè¯·å…ˆçˆ¬å–ä¸€äº›å†…å®¹ã€‚")
                    continue
                print(f"æ•°æ®åº“ä¸­å…±æœ‰ {len(db_posts)} ä¸ªå¸–å­å’Œ {len(db_comments)} æ¡è¯„è®º")
                
                # æ™ºèƒ½åˆ†æ
                negative_stats, detailed_stats = analyze_negative_content(db_posts, db_comments, NEGATIVE_KEYWORDS)
                
                # è¾“å‡ºä»£è¡¨æ€§è´Ÿé¢æ–‡æœ¬
                output_representative_negative_texts(negative_stats)
                
                # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
                generate_analysis_report(negative_stats, detailed_stats)
                
                # å¯¼å‡ºåˆ†æç»“æœ
                export_analysis_results(negative_stats, detailed_stats)
                
                # å¯è§†åŒ–åˆ†æç»“æœ
                visualize_negative_stats(negative_stats, detailed_stats)
                
                # ç”Ÿæˆå„é¢†åŸŸè¯äº‘å›¾
                generate_wordcloud(db_posts, db_comments, negative_stats)
                return
                
            except Exception as e:
                print(f"åˆ†æå¤±è´¥ï¼š{e}")
                import traceback
                traceback.print_exc()
                continue
         
        elif mode == "7":
            print("\nå¼€å§‹æ¸…ç†æ•°æ®åº“é‡å¤æ•°æ®...")
            try:
                clean_database()
                print("âœ… æ•°æ®åº“æ¸…ç†å®Œæˆ")
            except Exception as e:
                print(f"âŒ æ¸…ç†å¤±è´¥ï¼š{e}")
            continue
            
        elif mode == "8":
            print("\nâš ï¸  è­¦å‘Šï¼šæ­¤æ“ä½œå°†åˆ é™¤æ‰€æœ‰æ•°æ®åº“æ•°æ®ï¼")
            confirm = input("è¯·è¾“å…¥ 'YES' ç¡®è®¤æ¸…ç©ºæ‰€æœ‰æ•°æ®ï¼š").strip()
            if confirm == "YES":
                try:
                    clear_all_data()
                    print("âœ… æ‰€æœ‰æ•°æ®å·²æ¸…ç©º")
                except Exception as e:
                    print(f"âŒ æ¸…ç©ºå¤±è´¥ï¼š{e}")
            else:
                print("æ“ä½œå·²å–æ¶ˆ")
            continue
            
        elif mode == "9":
            show_keyword_statistics()
            continue
            
        elif mode == "0":
            print("ğŸ‘‹ é€€å‡ºç¨‹åº")
            return
        else:
            print("è¾“å…¥æœ‰è¯¯ï¼Œè¯·é‡æ–°è¾“å…¥0-9ä¹‹é—´çš„æ•°å­—ã€‚")
            continue

    # å¦‚æœæˆåŠŸçˆ¬å–äº†æ•°æ®ï¼Œè¿›è¡Œåˆ†æ
    print("\n" + "="*50)
    print("å¼€å§‹åˆ†ææ•°æ®...")
    print("="*50)
    
    try:
        # ä»æ•°æ®åº“è¯»å–æ‰€æœ‰æ•°æ®ï¼ˆç¡®ä¿åˆ†æçš„æ˜¯å…¨é‡æ•°æ®ï¼‰
        db_posts, db_comments = fetch_all_posts_and_comments()
        
        if not db_posts:
            print("æ•°æ®åº“ä¸­æ²¡æœ‰æ•°æ®å¯ä¾›åˆ†æã€‚")
            return
            
        print(f"æ•°æ®åº“ä¸­å…±æœ‰ {len(db_posts)} ä¸ªå¸–å­å’Œ {len(db_comments)} æ¡è¯„è®º")
        
        # è¿›è¡Œæ™ºèƒ½è´Ÿé¢èˆ†æƒ…åˆ†æ
        negative_stats, detailed_stats = analyze_negative_content(db_posts, db_comments, NEGATIVE_KEYWORDS)
        
        # è¾“å‡ºä»£è¡¨æ€§è´Ÿé¢æ–‡æœ¬
        output_representative_negative_texts(negative_stats)
        
        # ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š
        generate_analysis_report(negative_stats, detailed_stats)
        
        # å¯¼å‡ºåˆ†æç»“æœåˆ°æ–‡ä»¶
        export_analysis_results(negative_stats, detailed_stats)
        
        print("\nğŸ“Š æ­£åœ¨ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        # å¯è§†åŒ–åˆ†æç»“æœ
        visualize_negative_stats(negative_stats, detailed_stats)
        
        # ç”Ÿæˆå„é¢†åŸŸè¯äº‘å›¾
        generate_wordcloud(db_posts, db_comments, negative_stats)
        
    except Exception as e:
        print(f"åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š{e}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    main()





